{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenwei/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/wenwei/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, mean_squared_error, \n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import shap\n",
    "import itertools\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# LightGBM and XGBoost\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "\n",
    "# TensorFlow / Keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Displaying Images\n",
    "from IPython.display import Image\n",
    "\n",
    "# Ignore Warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"cleaned data/cleaned_data_split.csv\")\n",
    "\n",
    "predictive_cols_log = ['default_profile', 'default_profile_image',\n",
    "        'geo_enabled', 'deviation_from_humans', 'location', 'verified',\n",
    "        'account_age_days', 'is_description_na', 'is_lang_na', 'is_lang_en',\n",
    "       'is_location_unknown', 'creation_hour', 'creation_day_of_week',\n",
    "       'creation_month', 'creation_year', 'is_weekend', 'creation_quarter',\n",
    "       'part_of_day', 'creation_week_of_year', 'is_beginning_of_month',\n",
    "       'is_end_of_month', 'description_length', 'influencer_type',\n",
    "       'favourites_per_day', 'favourites_activity',\n",
    "       'mention_count', 'log_favourites_count', 'log_followers_count', 'log_friends_count',\n",
    "       'log_statuses_count', 'log_average_tweets_per_day',\n",
    "       'log_fol_to_friends_ratio', 'log_fol_to_tweets_ratio',\n",
    "       'log_friends_to_tweets_ratio','account_type']\n",
    "\n",
    "features = [col for col in predictive_cols_log if col not in ['id', 'account_type', 'X_fold']]\n",
    "target = 'account_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features.astype('float32').values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': self.features[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "class AccountTypeClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64], num_classes=2, dropout_rate=0.3):\n",
    "        super(AccountTypeClassifier, self).__init__()\n",
    "        \n",
    "        # Build layers dynamically\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(prev_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return self.output(x)\n",
    "\n",
    "def prepare_data(mds, predictive_cols, target_cols):\n",
    "    # Encode target variable if needed\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(mds[target_cols[0]])\n",
    "    \n",
    "    # Split data based on X_fold column\n",
    "    train_data = mds[mds['X_fold'] == 'train']\n",
    "    valid_data = mds[mds['X_fold'] == 'valid']\n",
    "    test_data = mds[mds['X_fold'] == 'test']\n",
    "    oot_data = mds[mds['X_fold'] == 'oot']\n",
    "    \n",
    "    # Prepare features and labels for each split\n",
    "    X_train = train_data[predictive_cols]\n",
    "    y_train = y[mds['X_fold'] == 'train']\n",
    "    \n",
    "    X_valid = valid_data[predictive_cols]\n",
    "    y_valid = y[mds['X_fold'] == 'valid']\n",
    "    \n",
    "    X_test = test_data[predictive_cols]\n",
    "    y_test = y[mds['X_fold'] == 'test']\n",
    "    \n",
    "    X_oot = oot_data[predictive_cols]\n",
    "    y_oot = y[mds['X_fold'] == 'oot']\n",
    "    \n",
    "    return (X_train, y_train), (X_valid, y_valid), (X_test, y_test), (X_oot, y_oot), le\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_acc = correct_train / total_train\n",
    "        val_acc = correct_val / total_val\n",
    "        \n",
    "        # Save metrics\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model.state_dict()\n",
    "    \n",
    "    return best_model, history\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy, all_preds, all_labels\n",
    "\n",
    "# Example usage\n",
    "def main(mds, predictive_cols, target_cols, params=None):\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'batch_size': 32,\n",
    "            'hidden_dims': [128, 64],\n",
    "            'learning_rate': 0.001,\n",
    "            'num_epochs': 10,\n",
    "            'dropout_rate': 0.3\n",
    "        }\n",
    "    \n",
    "    # Prepare data\n",
    "    (X_train, y_train), (X_valid, y_valid), (X_test, y_test), (X_oot, y_oot), le = prepare_data(\n",
    "        mds, predictive_cols, target_cols\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CustomDataset(X_train, pd.Series(y_train))\n",
    "    valid_dataset = CustomDataset(X_valid, pd.Series(y_valid))\n",
    "    test_dataset = CustomDataset(X_test, pd.Series(y_test))\n",
    "    oot_dataset = CustomDataset(X_oot, pd.Series(y_oot))\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=params['batch_size'])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'])\n",
    "    oot_loader = DataLoader(oot_dataset, batch_size=params['batch_size'])\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AccountTypeClassifier(\n",
    "        input_dim=len(predictive_cols),\n",
    "        hidden_dims=params['hidden_dims'],\n",
    "        num_classes=len(le.classes_),\n",
    "        dropout_rate=params['dropout_rate']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "    \n",
    "    # Train model\n",
    "    best_model_state, history = train_model(\n",
    "        model, train_loader, valid_loader, criterion, optimizer,\n",
    "        num_epochs=params['num_epochs'], device=device\n",
    "    )\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Evaluate on test and OOT sets\n",
    "    test_loss, test_acc, test_preds, test_labels = evaluate_model(\n",
    "        model, test_loader, criterion, device\n",
    "    )\n",
    "    oot_loss, oot_acc, oot_preds, oot_labels = evaluate_model(\n",
    "        model, oot_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'test_metrics': {'loss': test_loss, 'accuracy': test_acc, 'recall': recall_score(test_labels,test_preds), 'precision': precision_score(test_labels,test_preds), 'f1-score':f1_score(test_labels,test_preds)},\n",
    "        'oot_metrics': {'loss': oot_loss, 'accuracy': oot_acc, 'recall': recall_score(oot_labels, oot_preds), 'precision': precision_score(oot_labels, oot_preds), 'f1-score':f1_score(oot_labels, oot_preds)},\n",
    "        'label_encoder': le\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15]\n",
      "Train Loss: 0.5591, Train Acc: 0.7102\n",
      "Val Loss: 0.5860, Val Acc: 0.7065\n",
      "Epoch [2/15]\n",
      "Train Loss: 0.5008, Train Acc: 0.7611\n",
      "Val Loss: 0.4931, Val Acc: 0.7755\n",
      "Epoch [3/15]\n",
      "Train Loss: 0.4754, Train Acc: 0.7798\n",
      "Val Loss: 0.7045, Val Acc: 0.5755\n",
      "Epoch [4/15]\n",
      "Train Loss: 0.4699, Train Acc: 0.7844\n",
      "Val Loss: 0.4693, Val Acc: 0.7870\n",
      "Epoch [5/15]\n",
      "Train Loss: 0.4573, Train Acc: 0.7890\n",
      "Val Loss: 0.4495, Val Acc: 0.8061\n",
      "Epoch [6/15]\n",
      "Train Loss: 0.4534, Train Acc: 0.7944\n",
      "Val Loss: 0.4696, Val Acc: 0.7876\n",
      "Epoch [7/15]\n",
      "Train Loss: 0.4476, Train Acc: 0.7995\n",
      "Val Loss: 0.4434, Val Acc: 0.8044\n",
      "Epoch [8/15]\n",
      "Train Loss: 0.4475, Train Acc: 0.7972\n",
      "Val Loss: 0.4395, Val Acc: 0.8066\n",
      "Epoch [9/15]\n",
      "Train Loss: 0.4445, Train Acc: 0.8009\n",
      "Val Loss: 0.4458, Val Acc: 0.8063\n",
      "Epoch [10/15]\n",
      "Train Loss: 0.4415, Train Acc: 0.8029\n",
      "Val Loss: 0.4525, Val Acc: 0.8028\n",
      "Epoch [11/15]\n",
      "Train Loss: 0.4408, Train Acc: 0.8023\n",
      "Val Loss: 0.5506, Val Acc: 0.7077\n",
      "Epoch [12/15]\n",
      "Train Loss: 0.4345, Train Acc: 0.8048\n",
      "Val Loss: 0.4376, Val Acc: 0.8074\n",
      "Epoch [13/15]\n",
      "Train Loss: 0.4349, Train Acc: 0.8062\n",
      "Val Loss: 0.4509, Val Acc: 0.7991\n",
      "Epoch [14/15]\n",
      "Train Loss: 0.4295, Train Acc: 0.8077\n",
      "Val Loss: 0.4234, Val Acc: 0.8235\n",
      "Epoch [15/15]\n",
      "Train Loss: 0.4290, Train Acc: 0.8124\n",
      "Val Loss: 0.4350, Val Acc: 0.8226\n"
     ]
    }
   ],
   "source": [
    "# Running Model\n",
    "\n",
    "results = main(\n",
    "    mds=df,  \n",
    "    predictive_cols=features,  \n",
    "    target_cols=[target], \n",
    "    params={\n",
    "        'batch_size': 32,\n",
    "        'hidden_dims': [128, 64],\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 15,\n",
    "        'dropout_rate': 0.3\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicability\n",
    "\n",
    "- Save Model\n",
    "- Save Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, output_dir, model_type, model_name='_best_model.pkl'):\n",
    "    \"\"\"Save the trained XGBoost model to a pickle file.\"\"\"\n",
    "    with open(f\"{output_dir}/{model_type + model_name}\", 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "save_model(results['model'], output_dir = 'output_files', model_type = 'nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        'model': 'RandomForest',\n",
    "        'test_metrics': results['test_metrics'],\n",
    "        'oot_metrics': results['oot_metrics']\n",
    "    }\n",
    "]\n",
    "df_results = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv('output_files/model_results_nn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
