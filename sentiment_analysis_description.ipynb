{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Description of Twitter Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eldricksim/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/eldricksim/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>description</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>geo_enabled</th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>profile_background_image_url</th>\n",
       "      <th>profile_image_url</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>average_tweets_per_day</th>\n",
       "      <th>account_age_days</th>\n",
       "      <th>account_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-10-15 21:32:11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Blame @xaiax, Inspired by @MakingInvisible, us...</td>\n",
       "      <td>4</td>\n",
       "      <td>1589</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>787405734442958848</td>\n",
       "      <td>en</td>\n",
       "      <td>unknown</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/7874121826...</td>\n",
       "      <td>best_in_dumbest</td>\n",
       "      <td>11041</td>\n",
       "      <td>False</td>\n",
       "      <td>7.870</td>\n",
       "      <td>1403</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-11-09 05:01:30</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Photographing the American West since 1980. I ...</td>\n",
       "      <td>536</td>\n",
       "      <td>860</td>\n",
       "      <td>880</td>\n",
       "      <td>False</td>\n",
       "      <td>796216118331310080</td>\n",
       "      <td>en</td>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/8023296328...</td>\n",
       "      <td>CJRubinPhoto</td>\n",
       "      <td>252</td>\n",
       "      <td>False</td>\n",
       "      <td>0.183</td>\n",
       "      <td>1379</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-17 05:34:27</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Scruffy looking nerf herder and @twitch broadc...</td>\n",
       "      <td>3307</td>\n",
       "      <td>172</td>\n",
       "      <td>594</td>\n",
       "      <td>True</td>\n",
       "      <td>875949740503859204</td>\n",
       "      <td>en</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1278890453...</td>\n",
       "      <td>SVGEGENT</td>\n",
       "      <td>1001</td>\n",
       "      <td>False</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1159</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-07-21 13:32:25</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Wife.Godmother.Friend.Feline Fanatic! Assistan...</td>\n",
       "      <td>8433</td>\n",
       "      <td>517</td>\n",
       "      <td>633</td>\n",
       "      <td>True</td>\n",
       "      <td>756119643622735875</td>\n",
       "      <td>en</td>\n",
       "      <td>Birmingham, AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1284884924...</td>\n",
       "      <td>TinkerVHELPK5</td>\n",
       "      <td>1324</td>\n",
       "      <td>False</td>\n",
       "      <td>0.889</td>\n",
       "      <td>1489</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-01-15 16:32:35</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Loan coach at @mancity &amp; Aspiring DJ</td>\n",
       "      <td>88</td>\n",
       "      <td>753678</td>\n",
       "      <td>116</td>\n",
       "      <td>True</td>\n",
       "      <td>464781334</td>\n",
       "      <td>en</td>\n",
       "      <td>England, United Kingdom</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/9952566258...</td>\n",
       "      <td>JoleonLescott</td>\n",
       "      <td>4202</td>\n",
       "      <td>True</td>\n",
       "      <td>1.339</td>\n",
       "      <td>3138</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           created_at  default_profile  default_profile_image  \\\n",
       "0           0  2016-10-15 21:32:11            False                  False   \n",
       "1           1  2016-11-09 05:01:30            False                  False   \n",
       "2           2  2017-06-17 05:34:27            False                  False   \n",
       "3           3  2016-07-21 13:32:25             True                  False   \n",
       "4           4  2012-01-15 16:32:35            False                  False   \n",
       "\n",
       "                                         description  favourites_count  \\\n",
       "0  Blame @xaiax, Inspired by @MakingInvisible, us...                 4   \n",
       "1  Photographing the American West since 1980. I ...               536   \n",
       "2  Scruffy looking nerf herder and @twitch broadc...              3307   \n",
       "3  Wife.Godmother.Friend.Feline Fanatic! Assistan...              8433   \n",
       "4               Loan coach at @mancity & Aspiring DJ                88   \n",
       "\n",
       "   followers_count  friends_count  geo_enabled                  id lang  \\\n",
       "0             1589              4        False  787405734442958848   en   \n",
       "1              860            880        False  796216118331310080   en   \n",
       "2              172            594         True  875949740503859204   en   \n",
       "3              517            633         True  756119643622735875   en   \n",
       "4           753678            116         True           464781334   en   \n",
       "\n",
       "                  location                      profile_background_image_url  \\\n",
       "0                  unknown  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
       "1           Estados Unidos  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
       "2          Los Angeles, CA  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
       "3           Birmingham, AL                                               NaN   \n",
       "4  England, United Kingdom  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
       "\n",
       "                                   profile_image_url      screen_name  \\\n",
       "0  http://pbs.twimg.com/profile_images/7874121826...  best_in_dumbest   \n",
       "1  http://pbs.twimg.com/profile_images/8023296328...     CJRubinPhoto   \n",
       "2  http://pbs.twimg.com/profile_images/1278890453...         SVGEGENT   \n",
       "3  http://pbs.twimg.com/profile_images/1284884924...    TinkerVHELPK5   \n",
       "4  http://pbs.twimg.com/profile_images/9952566258...    JoleonLescott   \n",
       "\n",
       "   statuses_count  verified  average_tweets_per_day  account_age_days  \\\n",
       "0           11041     False                   7.870              1403   \n",
       "1             252     False                   0.183              1379   \n",
       "2            1001     False                   0.864              1159   \n",
       "3            1324     False                   0.889              1489   \n",
       "4            4202      True                   1.339              3138   \n",
       "\n",
       "  account_type  \n",
       "0          bot  \n",
       "1        human  \n",
       "2        human  \n",
       "3        human  \n",
       "4        human  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libraries and Data\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"twitter_human_bots_dataset.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# Check cuda status\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description: I love this!\n",
      "Prediction: [{'label': 'LABEL_1', 'score': 0.46820929646492004}]\n",
      "\n",
      "Description: Buy followers, likes, and retweets!\n",
      "Prediction: [{'label': 'LABEL_1', 'score': 0.4000426232814789}]\n",
      "\n",
      "Description: I'm so happy.\n",
      "Prediction: [{'label': 'LABEL_1', 'score': 0.43870222568511963}]\n",
      "\n",
      "Description: Check out this amazing deal!\n",
      "Prediction: [{'label': 'LABEL_1', 'score': 0.46263229846954346}]\n",
      "\n",
      "Description: It's okay.\n",
      "Prediction: [{'label': 'LABEL_1', 'score': 0.42986100912094116}]\n",
      "\n",
      "Description: Absolutely fantastic!\n",
      "Prediction: [{'label': 'LABEL_1', 'score': 0.4574173092842102}]\n",
      "\n",
      "Description: Not good at all.\n",
      "Prediction: [{'label': 'LABEL_1', 'score': 0.44880273938179016}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"  # You can choose other models like \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Initialize pipeline\n",
    "bot_detection_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=device)\n",
    "\n",
    "# Sample Twitter descriptions\n",
    "descriptions = [\n",
    "    \"I love this!\",\n",
    "    \"Buy followers, likes, and retweets!\",\n",
    "    \"I'm so happy.\",\n",
    "    \"Check out this amazing deal!\",\n",
    "    \"It's okay.\",\n",
    "    \"Absolutely fantastic!\",\n",
    "    \"Not good at all.\"\n",
    "]\n",
    "\n",
    "# Analyze descriptions\n",
    "for description in descriptions:\n",
    "    result = bot_detection_pipeline(description)\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Prediction: {result}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input': self.encodings.iloc[idx],\n",
    "            'label': self.labels.iloc[idx]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomDataset' object has no attribute '_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m\n\u001b[1;32m     20\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     21\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     22\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     23\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     24\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:2081\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2079\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2080\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 2081\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   2083\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:912\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    910\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m--> 912\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remove_unused_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_collator_with_removed_columns(data_collator, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/trainer.py:828\u001b[0m, in \u001b[0;36mTrainer._remove_unused_columns\u001b[0;34m(self, dataset, description)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_signature_columns_if_needed()\n\u001b[1;32m    826\u001b[0m signature_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signature_columns\n\u001b[0;32m--> 828\u001b[0m ignored_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(signature_columns))\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ignored_columns) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    830\u001b[0m     dset_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m description \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/datasets/arrow_dataset.py:1809\u001b[0m, in \u001b[0;36mDataset.column_names\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1797\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcolumn_names\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Names of the columns in the dataset.\u001b[39;00m\n\u001b[1;32m   1799\u001b[0m \n\u001b[1;32m   1800\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[38;5;241m.\u001b[39mcolumn_names\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CustomDataset' object has no attribute '_data'"
     ]
    }
   ],
   "source": [
    "# Split the data into training and evaluation datasets\n",
    "X = df[\"description\"]\n",
    "y = df[\"account_type\"]\n",
    "\n",
    "X_train, y_train, X_eval, y_eval = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "eval_dataset = CustomDataset(X_eval, y_eval)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
