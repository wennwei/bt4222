{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"source data/twitter_human_bots_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'created_at' to datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "\n",
    "# Calculate Account Age\n",
    "current_time = datetime.now()\n",
    "df['account_age_days'] = (current_time - df['created_at']).dt.days\n",
    "\n",
    "# Time-Based Features\n",
    "df['creation_hour'] = df['created_at'].dt.hour\n",
    "df['creation_day_of_week'] = df['created_at'].dt.dayofweek\n",
    "df['creation_month'] = df['created_at'].dt.month\n",
    "df['creation_year'] = df['created_at'].dt.year\n",
    "df['creation_quarter'] = df['created_at'].dt.quarter\n",
    "df['is_weekend'] = df['creation_day_of_week'] >= 5\n",
    "df['creation_week_of_year'] = df['created_at'].dt.isocalendar().week\n",
    "df['is_beginning_of_month'] = df['created_at'].dt.day <= 5\n",
    "df['is_end_of_month'] = df['created_at'].dt.day >= 26\n",
    "\n",
    "# Define part of day based on hour\n",
    "def part_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df['part_of_day'] = df['creation_hour'].apply(part_of_day)\n",
    "\n",
    "# Additional Features\n",
    "humans_mean = df[df['account_type'] == 'human']['average_tweets_per_day'].mean()\n",
    "humans_std = df[df['account_type'] == 'human']['average_tweets_per_day'].std()\n",
    "df['deviation_from_humans'] = (df['average_tweets_per_day'] - humans_mean) / humans_std\n",
    "\n",
    "# Description Length Feature\n",
    "df['description_length'] = df['description'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "\n",
    "# Followers/Friends Ratios\n",
    "df['followers_to_friends_ratio'] = df['followers_count'] / df['friends_count']\n",
    "df['followers_to_friends_ratio'].fillna(0, inplace=True)\n",
    "\n",
    "# Followers to Tweets Per Day Ratio\n",
    "df['followers_to_tweets_per_day_ratio'] = df['followers_count'] / df['average_tweets_per_day']\n",
    "df['followers_to_tweets_per_day_ratio'].fillna(0, inplace=True)\n",
    "\n",
    "# Mentions Count in Description\n",
    "import re\n",
    "\n",
    "def extract_mentions(description):\n",
    "    return re.findall(r'@\\w+', str(description))\n",
    "\n",
    "df['mentions'] = df['description'].apply(extract_mentions)\n",
    "df['mention_count'] = df['mentions'].apply(len)\n",
    "\n",
    "# Ensure any remaining NaN values are filled if necessary\n",
    "########################################################################\n",
    "# TO DISCUSS METHOD OF IMPUTATION\n",
    "########################################################################\n",
    "df.fillna(0, inplace=True)\n",
    "df.replace(np.inf,0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0                     created_at  default_profile  \\\n",
      "count  37438.000000                          37438     37438.000000   \n",
      "mean   18718.500000  2012-05-10 19:06:11.558710528         0.419894   \n",
      "min        0.000000            2006-07-05 19:52:46         0.000000   \n",
      "25%     9359.250000  2009-12-26 20:54:38.750000128         0.000000   \n",
      "50%    18718.500000            2011-10-27 02:04:41         0.000000   \n",
      "75%    28077.750000            2014-04-16 15:39:40         1.000000   \n",
      "max    37437.000000            2019-04-24 08:53:21         1.000000   \n",
      "std    10807.564026                            NaN         0.493548   \n",
      "\n",
      "       default_profile_image  favourites_count  followers_count  \\\n",
      "count           37438.000000      37438.000000     3.743800e+04   \n",
      "mean                0.014905      12302.062183     3.703098e+05   \n",
      "min                 0.000000          0.000000     0.000000e+00   \n",
      "25%                 0.000000        362.000000     3.500000e+01   \n",
      "50%                 0.000000       2066.000000     3.650000e+02   \n",
      "75%                 0.000000       8879.000000     8.440250e+03   \n",
      "max                 1.000000     885123.000000     1.216415e+08   \n",
      "std                 0.121173      33923.650237     2.470829e+06   \n",
      "\n",
      "       friends_count   geo_enabled            id          lang  ...  \\\n",
      "count   3.743800e+04  37438.000000  3.743800e+04  37438.000000  ...   \n",
      "mean    4.445925e+03      0.456141  1.221536e+17      9.959800  ...   \n",
      "min     0.000000e+00      0.000000  4.180000e+02      0.000000  ...   \n",
      "25%     3.700000e+01      0.000000  9.957306e+07      7.000000  ...   \n",
      "50%     2.960000e+02      0.000000  3.991474e+08     10.000000  ...   \n",
      "75%     8.930000e+02      1.000000  2.453826e+09     10.000000  ...   \n",
      "max     4.343060e+06      1.000000  1.120974e+18     48.000000  ...   \n",
      "std     4.954520e+04      0.498079  3.004313e+17      8.821715  ...   \n",
      "\n",
      "         is_weekend  creation_week_of_year  is_beginning_of_month  \\\n",
      "count  37438.000000                37438.0           37438.000000   \n",
      "mean       0.252337              25.293392               0.175570   \n",
      "min        0.000000                    1.0               0.000000   \n",
      "25%        0.000000                   13.0               0.000000   \n",
      "50%        0.000000                   25.0               0.000000   \n",
      "75%        1.000000                   37.0               0.000000   \n",
      "max        1.000000                   53.0               1.000000   \n",
      "std        0.434360              14.539285               0.380459   \n",
      "\n",
      "       is_end_of_month   part_of_day  deviation_from_humans  \\\n",
      "count     37438.000000  37438.000000           37438.000000   \n",
      "mean          0.172018      1.737593               0.012280   \n",
      "min           0.000000      0.000000              -0.460227   \n",
      "25%           0.000000      1.000000              -0.426289   \n",
      "50%           0.000000      2.000000              -0.358206   \n",
      "75%           0.000000      3.000000              -0.081216   \n",
      "max           1.000000      3.000000              86.548835   \n",
      "std           0.377401      1.175178               1.685264   \n",
      "\n",
      "       description_length  followers_to_friends_ratio  \\\n",
      "count        37438.000000                3.743800e+04   \n",
      "mean            67.017202                9.123033e+03   \n",
      "min              0.000000                0.000000e+00   \n",
      "25%             14.000000                1.548705e-01   \n",
      "50%             58.000000                8.281594e-01   \n",
      "75%            118.000000                5.987312e+00   \n",
      "max            191.000000                5.547370e+07   \n",
      "std             55.217549                3.898016e+05   \n",
      "\n",
      "       followers_to_tweets_per_day_ratio  mention_count  \n",
      "count                       3.743800e+04   37438.000000  \n",
      "mean                        9.329537e+05       0.299135  \n",
      "min                         0.000000e+00       0.000000  \n",
      "25%                         4.602353e+01       0.000000  \n",
      "50%                         2.146013e+02       0.000000  \n",
      "75%                         2.729065e+03       0.000000  \n",
      "max                         7.379663e+09      12.000000  \n",
      "std                         5.338276e+07       0.849194  \n",
      "\n",
      "[8 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Encoding Categorical Features\n",
    "df['account_type'] = df['account_type'].map({'human': 0, 'bot': 1})\n",
    "\n",
    "encode_cols = ['default_profile', 'default_profile_image', 'geo_enabled', 'lang', 'location', 'verified',\n",
    "               'creation_year', 'is_weekend', 'is_beginning_of_month', 'is_end_of_month', 'part_of_day']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in encode_cols:\n",
    "    df[col] = label_encoder.fit_transform(df[col].astype(str))  # Convert to string to handle NaNs if any\n",
    "\n",
    "\n",
    "# Define Feature Columns and Target\n",
    "id_col = ['id']\n",
    "labels = ['account_type']\n",
    "predictive_cols = ['default_profile', 'default_profile_image', 'favourites_count', 'followers_count', 'friends_count',\n",
    "                   'geo_enabled', 'lang', 'location', 'statuses_count', 'verified', 'average_tweets_per_day', \n",
    "                   'account_age_days', 'creation_hour', 'creation_day_of_week', 'creation_month', 'creation_year',\n",
    "                   'creation_quarter', 'is_weekend', 'creation_week_of_year', 'is_beginning_of_month', \n",
    "                   'is_end_of_month', 'part_of_day', 'deviation_from_humans', 'description_length', \n",
    "                   'followers_to_friends_ratio', 'followers_to_tweets_per_day_ratio', 'mention_count','account_type']\n",
    "\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "predictive_cols = ['default_profile', 'default_profile_image', 'favourites_count', 'followers_count', 'friends_count',\n",
    "                   'geo_enabled', 'lang', 'location', 'statuses_count', 'verified', 'average_tweets_per_day', \n",
    "                   'account_age_days', 'creation_hour', 'creation_day_of_week', 'creation_month', 'creation_year',\n",
    "                   'creation_quarter', 'is_weekend', 'creation_week_of_year', 'is_beginning_of_month', \n",
    "                   'is_end_of_month', 'part_of_day', 'deviation_from_humans', 'description_length', \n",
    "                   'followers_to_friends_ratio', 'followers_to_tweets_per_day_ratio', 'mention_count', 'description', 'account_type']\n",
    "\n",
    "# Set cutoff date for training/validation split\n",
    "cutoff_date = pd.to_datetime('2017-01-01')\n",
    "df['date'] = pd.to_datetime(df['created_at'])\n",
    "\n",
    "# Define columns to keep\n",
    "id_cols = ['id']\n",
    "# Assuming predictive_cols is already defined\n",
    "columns_to_keep = id_cols + predictive_cols\n",
    "df_filtered = df[columns_to_keep + ['date']]\n",
    "\n",
    "# Split data based on cutoff date\n",
    "oot = df_filtered[df_filtered['date'] >= cutoff_date].set_index('id')\n",
    "df_model = df_filtered[df_filtered['date'] < cutoff_date].set_index('id')\n",
    "\n",
    "# Drop date column as it's no longer needed\n",
    "oot = oot.drop('date', axis=1)\n",
    "df_model = df_model.drop('date', axis=1)\n",
    "# print(df_model.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # Check if the input is not a string, return an empty string if so\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Replace links with {link} and videos with [video]\n",
    "    text = re.sub(r'{link}', '', text)\n",
    "    text = re.sub(r\"\\[video\\]\", '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Process text\n",
    "texts = df_model['description'].apply(lambda x: process_text(x))\n",
    "\n",
    "vocab_size = 14225\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Tokenize text\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences to ensure they all have the same length\n",
    "# Using 75th percentile of description lengths as max_length\n",
    "description_lengths = df_model['description'].apply(lambda x: len(str(x).split()))\n",
    "max_length = description_lengths.quantile(0.75).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df_model['account_type']\n",
    "x_text = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "x_num = df_model.drop(columns=['description', 'account_type'])\n",
    "\n",
    "x_text_train, x_text_test, x_num_train, x_num_test, y_train, y_test = train_test_split(\n",
    "    x_text, x_num, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, text_data, num_data, labels):\n",
    "        self.text_data = torch.tensor(text_data, dtype=torch.long)\n",
    "        self.num_data = torch.tensor(num_data.astype('float32').values, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': self.text_data[idx],\n",
    "            'numerical': self.num_data[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = TwitterDataset(x_text_train, x_num_train, y_train)\n",
    "test_dataset = TwitterDataset(x_text_test, x_num_test, y_test)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TwitterBotDetector(nn.Module):\n",
    "    def __init__(self, num_numerical_features, embedding_dim, hidden_dim):\n",
    "        super(TwitterBotDetector, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim + num_numerical_features, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x_text, x_num):\n",
    "        x_embed = self.embedding(x_text)\n",
    "        x_embed = x_embed.mean(dim=1)  # Average embeddings over the sequence\n",
    "        x = torch.cat((x_embed, x_num), dim=1)  # Concatenate text and numerical features\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 64\n",
    "model = TwitterBotDetector(num_numerical_features=x_num.shape[1], \n",
    "                           embedding_dim=EMBEDDING_DIM, \n",
    "                           hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "# Binary Cross Entropy Loss for binary classification\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 28.5714\n",
      "Epoch [2/5], Loss: 14.2857\n",
      "Epoch [3/5], Loss: 28.5714\n",
      "Epoch [4/5], Loss: 42.8571\n",
      "Epoch [5/5], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        text_batch = batch['text']\n",
    "        num_batch = batch['numerical']\n",
    "        labels = batch['label']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(text_batch, num_batch)\n",
    "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 19.5907\n",
      "Test Accuracy: 0.7905\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "total_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        text_batch = batch['text']\n",
    "        num_batch = batch['numerical']\n",
    "        labels = batch['label']\n",
    "\n",
    "        outputs = model(text_batch, num_batch)\n",
    "        \n",
    "        # Calculate loss with raw outputs and actual labels\n",
    "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate accuracy with thresholded predictions\n",
    "        predicted = (outputs > 0.5).float()  # Threshold outputs to get binary predictions\n",
    "        correct += (predicted.view(-1) == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    print(f'Test Loss: {average_loss:.4f}')\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: lr=0.001, batch_size=16, embedding_dim=50, hidden_dim=64 -> Accuracy: 0.7473\n",
      "Params: lr=0.001, batch_size=16, embedding_dim=50, hidden_dim=128 -> Accuracy: 0.7460\n",
      "Params: lr=0.001, batch_size=16, embedding_dim=100, hidden_dim=64 -> Accuracy: 0.3154\n",
      "Params: lr=0.001, batch_size=16, embedding_dim=100, hidden_dim=128 -> Accuracy: 0.6846\n",
      "Params: lr=0.001, batch_size=32, embedding_dim=50, hidden_dim=64 -> Accuracy: 0.7819\n",
      "Params: lr=0.001, batch_size=32, embedding_dim=50, hidden_dim=128 -> Accuracy: 0.6846\n",
      "Params: lr=0.001, batch_size=32, embedding_dim=100, hidden_dim=64 -> Accuracy: 0.8024\n",
      "Params: lr=0.001, batch_size=32, embedding_dim=100, hidden_dim=128 -> Accuracy: 0.6846\n",
      "Params: lr=0.01, batch_size=16, embedding_dim=50, hidden_dim=64 -> Accuracy: 0.3154\n",
      "Params: lr=0.01, batch_size=16, embedding_dim=50, hidden_dim=128 -> Accuracy: 0.6846\n",
      "Params: lr=0.01, batch_size=16, embedding_dim=100, hidden_dim=64 -> Accuracy: 0.6846\n",
      "Params: lr=0.01, batch_size=16, embedding_dim=100, hidden_dim=128 -> Accuracy: 0.3154\n",
      "Params: lr=0.01, batch_size=32, embedding_dim=50, hidden_dim=64 -> Accuracy: 0.6846\n",
      "Params: lr=0.01, batch_size=32, embedding_dim=50, hidden_dim=128 -> Accuracy: 0.6846\n",
      "Params: lr=0.01, batch_size=32, embedding_dim=100, hidden_dim=64 -> Accuracy: 0.3154\n",
      "Params: lr=0.01, batch_size=32, embedding_dim=100, hidden_dim=128 -> Accuracy: 0.6846\n",
      "Best Accuracy: 0.8024 with parameters: {'learning_rate': 0.001, 'batch_size': 32, 'embedding_dim': 100, 'hidden_dim': 64}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'batch_size': [16, 32],\n",
    "    'embedding_dim': [50, 100],\n",
    "    'hidden_dim': [64, 128]\n",
    "}\n",
    "\n",
    "def train_and_evaluate(learning_rate, batch_size, embedding_dim, hidden_dim):\n",
    "    model = TwitterBotDetector(num_numerical_features=x_num_train.shape[1],\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               hidden_dim=hidden_dim)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            text_batch = batch['text']\n",
    "            num_batch = batch['numerical']\n",
    "            labels = batch['label']\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text_batch, num_batch)\n",
    "            loss = criterion(outputs, labels.float().view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            text_batch = batch['text']\n",
    "            num_batch = batch['numerical']\n",
    "            labels = batch['label']\n",
    "            outputs = model(text_batch, num_batch)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.view(-1) == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for params in itertools.product(*param_grid.values()):\n",
    "    learning_rate, batch_size, embedding_dim, hidden_dim = params\n",
    "    accuracy = train_and_evaluate(learning_rate, batch_size, embedding_dim, hidden_dim)\n",
    "    \n",
    "    print(f\"Params: lr={learning_rate}, batch_size={batch_size}, embedding_dim={embedding_dim}, hidden_dim={hidden_dim} -> Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'embedding_dim': embedding_dim,\n",
    "            'hidden_dim': hidden_dim\n",
    "        }\n",
    "\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f} with parameters: {best_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
