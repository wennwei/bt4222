{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT4222 Project: Twitter Bot Detection\n",
    "By:\n",
    "1. Cheong Wen Wei, A0233582E\n",
    "2. Lee Jun Wei, A0230329M\n",
    "3. Eldrick Sim Yen Kin, A0230114A\n",
    "4. Pwint Thiri Ko, A0239168U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "Use your own python\n",
    "I used 3.9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Saves the installed requirements to a requirements file\n",
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==1.0.0 (from -r requirements.txt (line 1))\n",
      "  Using cached accelerate-1.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting aiohappyeyeballs==2.4.3 (from -r requirements.txt (line 2))\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiohttp==3.10.9 (from -r requirements.txt (line 3))\n",
      "  Using cached aiohttp-3.10.9-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting aiosignal==1.3.1 (from -r requirements.txt (line 4))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: appnope==0.1.4 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.1.4)\n",
      "Requirement already satisfied: asttokens==2.4.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.4.1)\n",
      "Collecting async-timeout==4.0.3 (from -r requirements.txt (line 7))\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting attrs==24.2.0 (from -r requirements.txt (line 8))\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting certifi==2024.8.30 (from -r requirements.txt (line 9))\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer==3.4.0 (from -r requirements.txt (line 10))\n",
      "  Using cached charset_normalizer-3.4.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: comm==0.2.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (0.2.2)\n",
      "Collecting contourpy==1.3.0 (from -r requirements.txt (line 12))\n",
      "  Using cached contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.4 kB)\n",
      "Collecting cycler==0.12.1 (from -r requirements.txt (line 13))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting datasets==3.0.1 (from -r requirements.txt (line 14))\n",
      "  Using cached datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting debugpy==1.8.6 (from -r requirements.txt (line 15))\n",
      "  Using cached debugpy-1.8.6-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (5.1.1)\n",
      "Collecting dill==0.3.8 (from -r requirements.txt (line 17))\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: exceptiongroup==1.2.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (1.2.2)\n",
      "Requirement already satisfied: executing==2.1.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (2.1.0)\n",
      "Collecting filelock==3.16.1 (from -r requirements.txt (line 20))\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fonttools==4.54.1 (from -r requirements.txt (line 21))\n",
      "  Using cached fonttools-4.54.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (163 kB)\n",
      "Collecting frozenlist==1.4.1 (from -r requirements.txt (line 22))\n",
      "  Using cached frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting fsspec==2024.6.1 (from -r requirements.txt (line 23))\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting huggingface-hub==0.25.2 (from -r requirements.txt (line 24))\n",
      "  Using cached huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting idna==3.10 (from -r requirements.txt (line 25))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: importlib_metadata==8.5.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (8.5.0)\n",
      "Collecting importlib_resources==6.4.5 (from -r requirements.txt (line 27))\n",
      "  Using cached importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.18.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (8.18.1)\n",
      "Requirement already satisfied: jedi==0.19.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (0.19.1)\n",
      "Collecting Jinja2==3.1.4 (from -r requirements.txt (line 31))\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting joblib==1.4.2 (from -r requirements.txt (line 32))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 33)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 34)) (5.7.2)\n",
      "Collecting kiwisolver==1.4.7 (from -r requirements.txt (line 35))\n",
      "  Using cached kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Collecting MarkupSafe==3.0.1 (from -r requirements.txt (line 36))\n",
      "  Using cached MarkupSafe-3.0.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting matplotlib==3.9.2 (from -r requirements.txt (line 37))\n",
      "  Using cached matplotlib-3.9.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 38)) (0.1.7)\n",
      "Collecting mpmath==1.3.0 (from -r requirements.txt (line 39))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting multidict==6.1.0 (from -r requirements.txt (line 40))\n",
      "  Using cached multidict-6.1.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting multiprocess==0.70.16 (from -r requirements.txt (line 41))\n",
      "  Using cached multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 42)) (1.6.0)\n",
      "Collecting networkx==3.2.1 (from -r requirements.txt (line 43))\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting numpy==2.0.2 (from -r requirements.txt (line 44))\n",
      "  Using cached numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging==24.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 45)) (24.1)\n",
      "Collecting pandas==2.2.3 (from -r requirements.txt (line 46))\n",
      "  Using cached pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: parso==0.8.4 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 47)) (0.8.4)\n",
      "Collecting patsy==0.5.6 (from -r requirements.txt (line 48))\n",
      "  Using cached patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: pexpect==4.9.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 49)) (4.9.0)\n",
      "Collecting pillow==10.4.0 (from -r requirements.txt (line 50))\n",
      "  Using cached pillow-10.4.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 51)) (4.3.6)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.48 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 52)) (3.0.48)\n",
      "Collecting propcache==0.2.0 (from -r requirements.txt (line 53))\n",
      "  Using cached propcache-0.2.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: psutil==6.0.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 54)) (6.0.0)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 55)) (0.7.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 56)) (0.2.3)\n",
      "Collecting pyarrow==17.0.0 (from -r requirements.txt (line 57))\n",
      "  Using cached pyarrow-17.0.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: Pygments==2.18.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 58)) (2.18.0)\n",
      "Collecting pyparsing==3.1.4 (from -r requirements.txt (line 59))\n",
      "  Using cached pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 60)) (2.9.0.post0)\n",
      "Collecting pytz==2024.2 (from -r requirements.txt (line 61))\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting PyYAML==6.0.2 (from -r requirements.txt (line 62))\n",
      "  Using cached PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: pyzmq==26.2.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 63)) (26.2.0)\n",
      "Collecting regex==2024.9.11 (from -r requirements.txt (line 64))\n",
      "  Using cached regex-2024.9.11-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests==2.32.3 (from -r requirements.txt (line 65))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors==0.4.5 (from -r requirements.txt (line 66))\n",
      "  Using cached safetensors-0.4.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting scikit-learn==1.5.2 (from -r requirements.txt (line 67))\n",
      "  Using cached scikit_learn-1.5.2-cp39-cp39-macosx_12_0_arm64.whl.metadata (13 kB)\n",
      "Collecting scipy==1.13.1 (from -r requirements.txt (line 68))\n",
      "  Using cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting scorecardpy==0.1.9.7 (from -r requirements.txt (line 69))\n",
      "  Using cached scorecardpy-0.1.9.7-py3-none-any.whl\n",
      "Requirement already satisfied: six==1.16.0 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 70)) (1.16.0)\n",
      "Requirement already satisfied: stack-data==0.6.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 71)) (0.6.3)\n",
      "Collecting statsmodels==0.14.4 (from -r requirements.txt (line 72))\n",
      "  Using cached statsmodels-0.14.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting sympy==1.13.3 (from -r requirements.txt (line 73))\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting threadpoolctl==3.5.0 (from -r requirements.txt (line 74))\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tokenizers==0.20.0 (from -r requirements.txt (line 75))\n",
      "  Using cached tokenizers-0.20.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting torch==2.4.1 (from -r requirements.txt (line 76))\n",
      "  Using cached torch-2.4.1-cp39-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: tornado==6.4.1 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 77)) (6.4.1)\n",
      "Collecting tqdm==4.66.5 (from -r requirements.txt (line 78))\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: traitlets==5.14.3 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 79)) (5.14.3)\n",
      "Collecting transformers==4.45.2 (from -r requirements.txt (line 80))\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: typing_extensions==4.12.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 81)) (4.12.2)\n",
      "Collecting tzdata==2024.2 (from -r requirements.txt (line 82))\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting urllib3==2.2.3 (from -r requirements.txt (line 83))\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: wcwidth==0.2.13 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 84)) (0.2.13)\n",
      "Collecting xxhash==3.5.0 (from -r requirements.txt (line 85))\n",
      "  Using cached xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting yarl==1.14.0 (from -r requirements.txt (line 86))\n",
      "  Using cached yarl-1.14.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: zipp==3.20.2 in ./.venv/lib/python3.9/site-packages (from -r requirements.txt (line 87)) (3.20.2)\n",
      "Using cached accelerate-1.0.0-py3-none-any.whl (330 kB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiohttp-3.10.9-cp39-cp39-macosx_11_0_arm64.whl (392 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
      "Using cached contourpy-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (249 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "Using cached debugpy-1.8.6-py2.py3-none-any.whl (5.2 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fonttools-4.54.1-cp39-cp39-macosx_11_0_arm64.whl (2.3 MB)\n",
      "Using cached frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl (53 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached kiwisolver-1.4.7-cp39-cp39-macosx_11_0_arm64.whl (64 kB)\n",
      "Using cached MarkupSafe-3.0.1-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached matplotlib-3.9.2-cp39-cp39-macosx_11_0_arm64.whl (7.8 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.1.0-cp39-cp39-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached numpy-2.0.2-cp39-cp39-macosx_14_0_arm64.whl (5.3 MB)\n",
      "Using cached pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "Using cached pillow-10.4.0-cp39-cp39-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached propcache-0.2.0-cp39-cp39-macosx_11_0_arm64.whl (46 kB)\n",
      "Using cached pyarrow-17.0.0-cp39-cp39-macosx_11_0_arm64.whl (27.2 MB)\n",
      "Using cached pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached regex-2024.9.11-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached safetensors-0.4.5-cp39-cp39-macosx_11_0_arm64.whl (383 kB)\n",
      "Using cached scikit_learn-1.5.2-cp39-cp39-macosx_12_0_arm64.whl (11.0 MB)\n",
      "Using cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "Using cached statsmodels-0.14.4-cp39-cp39-macosx_11_0_arm64.whl (9.9 MB)\n",
      "Using cached sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.20.0-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\n",
      "Using cached torch-2.4.1-cp39-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached yarl-1.14.0-cp39-cp39-macosx_11_0_arm64.whl (86 kB)\n",
      "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, tqdm, threadpoolctl, sympy, safetensors, regex, PyYAML, pyparsing, propcache, pillow, numpy, networkx, multidict, MarkupSafe, kiwisolver, joblib, importlib_resources, idna, fsspec, frozenlist, fonttools, filelock, dill, debugpy, cycler, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, yarl, scipy, requests, pyarrow, patsy, pandas, multiprocess, Jinja2, contourpy, aiosignal, torch, statsmodels, scikit-learn, matplotlib, huggingface-hub, aiohttp, tokenizers, scorecardpy, accelerate, transformers, datasets\n",
      "  Attempting uninstall: debugpy\n",
      "    Found existing installation: debugpy 1.8.7\n",
      "    Uninstalling debugpy-1.8.7:\n",
      "      Successfully uninstalled debugpy-1.8.7\n",
      "Successfully installed Jinja2-3.1.4 MarkupSafe-3.0.1 PyYAML-6.0.2 accelerate-1.0.0 aiohappyeyeballs-2.4.3 aiohttp-3.10.9 aiosignal-1.3.1 async-timeout-4.0.3 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.4.0 contourpy-1.3.0 cycler-0.12.1 datasets-3.0.1 debugpy-1.8.6 dill-0.3.8 filelock-3.16.1 fonttools-4.54.1 frozenlist-1.4.1 fsspec-2024.6.1 huggingface-hub-0.25.2 idna-3.10 importlib_resources-6.4.5 joblib-1.4.2 kiwisolver-1.4.7 matplotlib-3.9.2 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.2.1 numpy-2.0.2 pandas-2.2.3 patsy-0.5.6 pillow-10.4.0 propcache-0.2.0 pyarrow-17.0.0 pyparsing-3.1.4 pytz-2024.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.13.1 scorecardpy-0.1.9.7 statsmodels-0.14.4 sympy-1.13.3 threadpoolctl-3.5.0 tokenizers-0.20.0 torch-2.4.1 tqdm-4.66.5 transformers-4.45.2 tzdata-2024.2 urllib3-2.2.3 xxhash-3.5.0 yarl-1.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installs the requirements from the requirements file\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data\n",
    "* Data Cleaning, Imputation, etc..\n",
    "* Train Test split (based on account creation date)\n",
    "* 2006-2017 used to train,test validate model\n",
    "* 2018-2019, out of sample testing, to test how well model performs with more recent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>description</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>geo_enabled</th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>profile_background_image_url</th>\n",
       "      <th>profile_image_url</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>average_tweets_per_day</th>\n",
       "      <th>account_age_days</th>\n",
       "      <th>account_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-10-15 21:32:11</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Blame @xaiax, Inspired by @MakingInvisible, us...</td>\n",
       "      <td>4</td>\n",
       "      <td>1589</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>787405734442958848</td>\n",
       "      <td>en</td>\n",
       "      <td>unknown</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/7874121826...</td>\n",
       "      <td>best_in_dumbest</td>\n",
       "      <td>11041</td>\n",
       "      <td>False</td>\n",
       "      <td>7.870</td>\n",
       "      <td>1403</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016-11-09 05:01:30</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Photographing the American West since 1980. I ...</td>\n",
       "      <td>536</td>\n",
       "      <td>860</td>\n",
       "      <td>880</td>\n",
       "      <td>False</td>\n",
       "      <td>796216118331310080</td>\n",
       "      <td>en</td>\n",
       "      <td>Estados Unidos</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/8023296328...</td>\n",
       "      <td>CJRubinPhoto</td>\n",
       "      <td>252</td>\n",
       "      <td>False</td>\n",
       "      <td>0.183</td>\n",
       "      <td>1379</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-06-17 05:34:27</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Scruffy looking nerf herder and @twitch broadc...</td>\n",
       "      <td>3307</td>\n",
       "      <td>172</td>\n",
       "      <td>594</td>\n",
       "      <td>True</td>\n",
       "      <td>875949740503859204</td>\n",
       "      <td>en</td>\n",
       "      <td>Los Angeles, CA</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1278890453...</td>\n",
       "      <td>SVGEGENT</td>\n",
       "      <td>1001</td>\n",
       "      <td>False</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1159</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-07-21 13:32:25</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Wife.Godmother.Friend.Feline Fanatic! Assistan...</td>\n",
       "      <td>8433</td>\n",
       "      <td>517</td>\n",
       "      <td>633</td>\n",
       "      <td>True</td>\n",
       "      <td>756119643622735875</td>\n",
       "      <td>en</td>\n",
       "      <td>Birmingham, AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/1284884924...</td>\n",
       "      <td>TinkerVHELPK5</td>\n",
       "      <td>1324</td>\n",
       "      <td>False</td>\n",
       "      <td>0.889</td>\n",
       "      <td>1489</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-01-15 16:32:35</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Loan coach at @mancity &amp; Aspiring DJ</td>\n",
       "      <td>88</td>\n",
       "      <td>753678</td>\n",
       "      <td>116</td>\n",
       "      <td>True</td>\n",
       "      <td>464781334</td>\n",
       "      <td>en</td>\n",
       "      <td>England, United Kingdom</td>\n",
       "      <td>http://abs.twimg.com/images/themes/theme1/bg.png</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/9952566258...</td>\n",
       "      <td>JoleonLescott</td>\n",
       "      <td>4202</td>\n",
       "      <td>True</td>\n",
       "      <td>1.339</td>\n",
       "      <td>3138</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           created_at  default_profile  default_profile_image  \\\n",
       "0           0  2016-10-15 21:32:11            False                  False   \n",
       "1           1  2016-11-09 05:01:30            False                  False   \n",
       "2           2  2017-06-17 05:34:27            False                  False   \n",
       "3           3  2016-07-21 13:32:25             True                  False   \n",
       "4           4  2012-01-15 16:32:35            False                  False   \n",
       "\n",
       "                                         description  favourites_count  \\\n",
       "0  Blame @xaiax, Inspired by @MakingInvisible, us...                 4   \n",
       "1  Photographing the American West since 1980. I ...               536   \n",
       "2  Scruffy looking nerf herder and @twitch broadc...              3307   \n",
       "3  Wife.Godmother.Friend.Feline Fanatic! Assistan...              8433   \n",
       "4               Loan coach at @mancity & Aspiring DJ                88   \n",
       "\n",
       "   followers_count  friends_count  geo_enabled                  id lang  \\\n",
       "0             1589              4        False  787405734442958848   en   \n",
       "1              860            880        False  796216118331310080   en   \n",
       "2              172            594         True  875949740503859204   en   \n",
       "3              517            633         True  756119643622735875   en   \n",
       "4           753678            116         True           464781334   en   \n",
       "\n",
       "                  location                      profile_background_image_url  \\\n",
       "0                  unknown  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
       "1           Estados Unidos  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
       "2          Los Angeles, CA  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
       "3           Birmingham, AL                                               NaN   \n",
       "4  England, United Kingdom  http://abs.twimg.com/images/themes/theme1/bg.png   \n",
       "\n",
       "                                   profile_image_url      screen_name  \\\n",
       "0  http://pbs.twimg.com/profile_images/7874121826...  best_in_dumbest   \n",
       "1  http://pbs.twimg.com/profile_images/8023296328...     CJRubinPhoto   \n",
       "2  http://pbs.twimg.com/profile_images/1278890453...         SVGEGENT   \n",
       "3  http://pbs.twimg.com/profile_images/1284884924...    TinkerVHELPK5   \n",
       "4  http://pbs.twimg.com/profile_images/9952566258...    JoleonLescott   \n",
       "\n",
       "   statuses_count  verified  average_tweets_per_day  account_age_days  \\\n",
       "0           11041     False                   7.870              1403   \n",
       "1             252     False                   0.183              1379   \n",
       "2            1001     False                   0.864              1159   \n",
       "3            1324     False                   0.889              1489   \n",
       "4            4202      True                   1.339              3138   \n",
       "\n",
       "  account_type  \n",
       "0          bot  \n",
       "1        human  \n",
       "2        human  \n",
       "3        human  \n",
       "4        human  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libraries and Data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "import scorecardpy as sc\n",
    "import warnings \n",
    "import re\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "df = pd.read_csv(\"twitter_human_bots_dataset.csv\")\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provided Features\n",
    "* created_at: timestamp\n",
    "    * Day account was created\n",
    "\n",
    "* default_profile: Boolean\n",
    "    * Indicating whether the account has a default profile\n",
    "\n",
    "* default_profile_image: Boolean\n",
    "    * Indicating whether the account has a default image profile\n",
    "\n",
    "* description: String\n",
    "    * User account description\n",
    "\n",
    "* favourites_count: Int\n",
    "    * Total number of favourite tweets\n",
    "\n",
    "* followers_count: Int\n",
    "    * Total number of followers\n",
    "\n",
    "* friend_count: Int\n",
    "    * Total number of friends (people who follow back)\n",
    "\n",
    "* geo_enabled: Boolean\n",
    "    * Indicating whether the account has the geographic location enabled\n",
    "\n",
    "* id: string\n",
    "    * unique identifier of the account\n",
    "\n",
    "* lang: string\n",
    "    * Language of the account\n",
    "\n",
    "* location: \n",
    "    * Location of the account\n",
    "\n",
    "* profile_background: string\n",
    "    * Profile background image url\n",
    "\n",
    "* profile_image_url: String\n",
    "    * Profile image URL\n",
    "\n",
    "* screen_name: string\n",
    "    * username\n",
    "\n",
    "* statuses_count: int\n",
    "    * Total number of tweets\n",
    "\n",
    "* verified: Boolean\n",
    "    * Indicating whether the account has been verified\n",
    "\n",
    "* average_tweets_per_day: int\n",
    "    * Average tweets posted per day (statuses_count / account_age_day)\n",
    "\n",
    "* account_age_day: int\n",
    "    * Account age measured in days\n",
    "\n",
    "* account_type: binary\n",
    "    * account type, bot or human\n",
    "\n",
    "### Transformed Features to consider:\n",
    "* Sentiment analysis for description\n",
    "* Number of mentions in description to other bot accounts\n",
    "* Length of description\n",
    "* Ratio Followers_count to friends count\n",
    "* Ratio of Followers_count to tweets per day\n",
    "* Ratio of friends to tweets per day\n",
    "* Ratio of tweets since account created\n",
    "* Time when account was created (Past midnight of timezone, obtained from location)\n",
    "* Standard deviation of avg tweets from avg tweets of bots\n",
    "\n",
    "### Total Features: 18 (provided) + 9 (created)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'created_at', 'default_profile', 'default_profile_image',\n",
       "       'description', 'favourites_count', 'followers_count', 'friends_count',\n",
       "       'geo_enabled', 'id', 'lang', 'location', 'profile_background_image_url',\n",
       "       'profile_image_url', 'screen_name', 'statuses_count', 'verified',\n",
       "       'average_tweets_per_day', 'account_age_days', 'account_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for Null Values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "* Feature Creation\n",
    "* Feature Reduction (using Weight of Evidence(woe) check for feature importance, Correlation)\n",
    "* Run feature selection using tree base algo (random forest or smth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006      13\n",
       "2007     354\n",
       "2008    1447\n",
       "2009    7598\n",
       "2010    4668\n",
       "2011    5550\n",
       "2012    4594\n",
       "2013    3013\n",
       "2014    2476\n",
       "2015    2140\n",
       "2016    2276\n",
       "2017    2445\n",
       "2018     835\n",
       "2019      29\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year'] = pd.to_datetime(df['created_at']).dt.year\n",
    "year_distribution = df['year'].value_counts().sort_index()\n",
    "year_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Length of Description Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    37438.000000\n",
      "mean        66.878092\n",
      "std         55.092550\n",
      "min          0.000000\n",
      "25%         14.000000\n",
      "50%         58.000000\n",
      "75%        118.000000\n",
      "max        190.000000\n",
      "Name: description_length, dtype: float64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "df['description_length'] = df['description'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
    "print(df['description_length'].describe())\n",
    "print(df['description_length'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of Followers_count to tweets per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3.640400e+04\n",
      "mean              inf\n",
      "std               NaN\n",
      "min      0.000000e+00\n",
      "25%      5.224889e-01\n",
      "50%      1.708792e+00\n",
      "75%      1.042999e+03\n",
      "max               inf\n",
      "Name: followers_to_friends_ratio, dtype: float64\n",
      "followers_to_friends_ratio\n",
      "inf            5376\n",
      "0.000000        221\n",
      "1.000000         85\n",
      "0.500000         64\n",
      "0.333333         59\n",
      "               ... \n",
      "3528.268145       1\n",
      "602.938852        1\n",
      "1.043967          1\n",
      "1.221818          1\n",
      "450.684211        1\n",
      "Name: count, Length: 26700, dtype: int64\n",
      "\n",
      "Number of NA: 1034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eldricksim/Desktop/NUS/Year 4/Y4S1/BT4222/Project/bt4222/.venv/lib/python3.9/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    }
   ],
   "source": [
    "df['followers_to_friends_ratio'] = df['followers_count'] / df['friends_count']\n",
    "print(df['followers_to_friends_ratio'].describe())\n",
    "print(df['followers_to_friends_ratio'].value_counts())\n",
    "print(f\"\\nNumber of NA: {df['followers_to_friends_ratio'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of Followers_count to tweets per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3.742300e+04\n",
      "mean              inf\n",
      "std               NaN\n",
      "min      0.000000e+00\n",
      "25%      4.651163e+01\n",
      "50%      2.173913e+02\n",
      "75%      2.794128e+03\n",
      "max               inf\n",
      "Name: followers_to_tweets_per_day_ratio, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eldricksim/Desktop/NUS/Year 4/Y4S1/BT4222/Project/bt4222/.venv/lib/python3.9/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    }
   ],
   "source": [
    "df['followers_to_tweets_per_day_ratio'] = df['followers_count'] / df['average_tweets_per_day']\n",
    "print(df['followers_to_tweets_per_day_ratio'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of friends to tweets per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    3.741100e+04\n",
      "mean              inf\n",
      "std               NaN\n",
      "min      0.000000e+00\n",
      "25%      1.927754e+01\n",
      "50%      1.250562e+02\n",
      "75%      4.652773e+02\n",
      "max               inf\n",
      "Name: friends_to_tweets_per_day_ratio, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eldricksim/Desktop/NUS/Year 4/Y4S1/BT4222/Project/bt4222/.venv/lib/python3.9/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    }
   ],
   "source": [
    "df['friends_to_tweets_per_day_ratio'] = df['friends_count'] / df['average_tweets_per_day']\n",
    "print(df['friends_to_tweets_per_day_ratio'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  \\\n",
      "0  Blame @xaiax, Inspired by @MakingInvisible, us...   \n",
      "1  Photographing the American West since 1980. I ...   \n",
      "2  Scruffy looking nerf herder and @twitch broadc...   \n",
      "3  Wife.Godmother.Friend.Feline Fanatic! Assistan...   \n",
      "4               Loan coach at @mancity & Aspiring DJ   \n",
      "\n",
      "                     mentions  mention_count  \n",
      "0  [@xaiax, @MakingInvisible]              2  \n",
      "1                          []              0  \n",
      "2           [@twitch, @gmail]              2  \n",
      "3                          []              0  \n",
      "4                  [@mancity]              1  \n"
     ]
    }
   ],
   "source": [
    "def extract_mentions(description):\n",
    "    mentions = re.findall(r'@\\w+', str(description))\n",
    "    return mentions\n",
    "\n",
    "# Apply the function to the description column to create a new column with the mentions\n",
    "df['mentions'] = df['description'].apply(extract_mentions)\n",
    "\n",
    "# Feature: Count of mentions\n",
    "df['mention_count'] = df['mentions'].apply(len)\n",
    "\n",
    "# Display the dataframe with the new columns\n",
    "print(df[['description', 'mentions', 'mention_count' ]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolving NA in created features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['followers_to_friends_ratio'].fillna(0, inplace=True)\n",
    "df['followers_to_tweets_per_day_ratio'].fillna(0, inplace=True)\n",
    "df['friends_to_tweets_per_day_ratio'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight of Evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How WOE works:\n",
    "Run the features you want to conduct weight of evidence on. For example, I put the features in temp_list and then created a dataframe containing only those features in df_tmp.\n",
    "\n",
    "* Step 1: Replace temp_list with your features\n",
    "* Step 2: Run code chunk below\n",
    "* Step 3: Look at the WOE score for each bin of each feature to interpret WOE results\n",
    "\n",
    "How to Interpret WOE scores\n",
    "Negative or Positive Values indicate the direction of the feature while the magnitude indicate the strength of the feature.\n",
    "E.g For the feature \"statuses_count\", if the woe score for one of its bin is 0.5 it indicates that for that bin, the feature \"statuses_count\" identifies many bots.\n",
    "Conversely if the woe score for one of its bin is -0.7, it means that the feature is not good a predicting bots.\n",
    "\n",
    "Also do look at the IV scores to determine the features contribution. Generally an IV of >0.02 indicates weak predictive power while >0.5 indicates strong predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] creating woe binning ...\n",
      "Binning for mention_count:\n",
      "        variable         bin  count  count_distr   good    bad   badprob  \\\n",
      "0  mention_count  [-inf,1.0)  31088     0.830386  19724  11364  0.365543   \n",
      "1  mention_count   [1.0,2.0)   3841     0.102596   3063    778  0.202551   \n",
      "2  mention_count   [2.0,inf)   2509     0.067017   2226    283  0.112794   \n",
      "\n",
      "        woe    bin_iv  total_iv breaks  is_special_values  \n",
      "0  0.148299  0.018694  0.149074    1.0              False  \n",
      "1 -0.670738  0.040137  0.149074    2.0              False  \n",
      "2 -1.362830  0.090243  0.149074    inf              False  \n",
      "Binning for favourites_count:\n",
      "           variable              bin  count  count_distr   good    bad  \\\n",
      "0  favourites_count    [-inf,3000.0)  22524     0.601635  11704  10820   \n",
      "1  favourites_count  [3000.0,5000.0)   2707     0.072306   2167    540   \n",
      "2  favourites_count     [5000.0,inf)  12207     0.326059  11142   1065   \n",
      "\n",
      "    badprob       woe    bin_iv  total_iv  breaks  is_special_values  \n",
      "0  0.480376  0.621151  0.250267  0.872914  3000.0              False  \n",
      "1  0.199483 -0.689845  0.029783  0.872914  5000.0              False  \n",
      "2  0.087245 -1.648062  0.592864  0.872914     inf              False  \n",
      "Binning for followers_count:\n",
      "          variable             bin  count  count_distr   good    bad  \\\n",
      "0  followers_count    [-inf,500.0)  20173     0.538838   9881  10292   \n",
      "1  followers_count  [500.0,1500.0)   4511     0.120493   3685    826   \n",
      "2  followers_count    [1500.0,inf)  12754     0.340670  11447   1307   \n",
      "\n",
      "    badprob       woe    bin_iv  total_iv  breaks  is_special_values  \n",
      "0  0.510187  0.740438  0.320829  0.903372   500.0              False  \n",
      "1  0.183108 -0.795746  0.064332  0.903372  1500.0              False  \n",
      "2  0.102478 -1.470308  0.518211  0.903372     inf              False  \n"
     ]
    }
   ],
   "source": [
    "temp_lst = ['favourites_count', 'followers_count','account_type','mention_count']\n",
    "#Transforming Bot/Human to 1s and 0s\n",
    "df['account_type'] = df['account_type'].replace({'bot': 1, 'human': 0})\n",
    "\n",
    "# Replace 'target' with actual column name\n",
    "y = df['account_type']  \n",
    "# Drop target to leave only features\n",
    "X = df.drop(columns=['account_type'])  \n",
    "\n",
    "df_tmp = df[temp_lst]\n",
    "# Conduct the binning process using woebin\n",
    "bins = sc.woebin(df_tmp, y='account_type', bins = 5)\n",
    "\n",
    "# Display the binning information (optional to view WoE values per variable)\n",
    "for key in bins.keys():\n",
    "    print(f\"Binning for {key}:\")\n",
    "    print(bins[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n        'created_at', 'default_profile', 'default_profile_image',\\n       'description', 'favourites_count', 'followers_count', 'friends_count',\\n       'geo_enabled', 'id', 'lang', 'location', 'profile_background_image_url',\\n       'profile_image_url', 'screen_name', 'statuses_count', 'verified',\\n       'average_tweets_per_day', 'account_age_days', 'account_type'\\n\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.shape\n",
    "\n",
    "#Transforming features\n",
    "#tagging of description, construct some sort NLA\n",
    "#text analysis for description\n",
    "#ratio followers_count to friends count\n",
    "#ratio of followers to tweets per day\n",
    "# ratio of friends to tweets per day\n",
    "#length of descriptions\n",
    "\"\"\"\n",
    "        'created_at', 'default_profile', 'default_profile_image',\n",
    "       'description', 'favourites_count', 'followers_count', 'friends_count',\n",
    "       'geo_enabled', 'id', 'lang', 'location', 'profile_background_image_url',\n",
    "       'profile_image_url', 'screen_name', 'statuses_count', 'verified',\n",
    "       'average_tweets_per_day', 'account_age_days', 'account_type'\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis for `description` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37438\n",
      "count    37438.000000\n",
      "mean        54.429163\n",
      "std         48.679166\n",
      "min          0.000000\n",
      "25%          7.000000\n",
      "50%         43.000000\n",
      "75%         96.000000\n",
      "max        160.000000\n",
      "Name: description, dtype: float64\n",
      "0        blame  inspired by  using cmu phonetic data to...\n",
      "1        photographing the american west since 1980 i s...\n",
      "2        scruffy looking nerf herder and  broadcaster\\n...\n",
      "3        wifegodmotherfriendfeline fanatic assistant pr...\n",
      "4                              loan coach at   aspiring dj\n",
      "                               ...                        \n",
      "37433    role stock taker past roles nanny sales assist...\n",
      "37434                       kingdom landlord freecornbread\n",
      "37435        bienvenid al twitter oficial de sergio dalma \n",
      "37436    just a good guy wrapped up in a bad system\\nac...\n",
      "37437                                                     \n",
      "Name: description, Length: 37438, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label     score\n",
      "0  LABEL_1  0.714407\n",
      "1  LABEL_1  0.716920\n",
      "2  LABEL_1  0.740677\n",
      "3  LABEL_1  0.705628\n",
      "4  LABEL_1  0.678854\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Preprocess the description column\n",
    "description = df['description'].fillna('').str.lower()\n",
    "description = description.str.replace(r'@\\w+', '', regex=True)\n",
    "description = description.str.replace(r'http\\S+', '', regex=True)\n",
    "description = description.str.replace('#', '', regex=True)\n",
    "description = description.str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
    "\n",
    "# Get description statistics\n",
    "print(len(description))\n",
    "description_lengths = description.apply(len)\n",
    "print(description_lengths.describe())\n",
    "print(description)\n",
    "\n",
    "# Initialise model\n",
    "model_name = 'bert-base-uncased'\n",
    "sentiment_model = pipeline('sentiment-analysis', model=model_name)\n",
    "\n",
    "# Get sentiment of the description\n",
    "sentiments = description.apply(lambda text: sentiment_model(text)[0])\n",
    "\n",
    "df1 = pd.DataFrame(sentiments.tolist())\n",
    "print(df1.head())\n",
    "# Takes 25 minutes to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "37433    1\n",
       "37434    1\n",
       "37435    1\n",
       "37436    1\n",
       "37437    1\n",
       "Name: sentiment_label, Length: 37438, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add sentiment label to original df\n",
    "df1['label'].value_counts()\n",
    "df.loc[df1['label']== 'LABEL_1', 'account_type'].value_counts()\n",
    "df['sentiment_label'] = df1['label'].map({'LABEL_0': 0, 'LABEL_1': 1})\n",
    "df['sentiment_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "### Tree based models\n",
    "* XGB, LGBM, RF, ETC, DT\n",
    "\n",
    "### Neural Networks\n",
    "* NN, autoencoders(?), GNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Shap for model explanability both local and global\n",
    "* Tune model on best model selected (either on precision/f1/recall and feature blend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on out of sample set\n",
    "* Get metrics(acc,precision,f1, etc..) from out of sample set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard(?) --enhancement\n",
    "* Transfer results to a dashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
