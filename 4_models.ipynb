{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wenwei/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/wenwei/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, TimeSeriesSplit\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, mean_squared_error, \n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import shap\n",
    "import itertools\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# LightGBM and XGBoost\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from xgboost import DMatrix\n",
    "\n",
    "# TensorFlow / Keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Displaying Images\n",
    "from IPython.display import Image\n",
    "\n",
    "# Ignore Warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import file here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(\"cleaned data/cleaned_data_split.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Model Pipeline\n",
    "\n",
    "### 1. Feature Engineering Pipeline\n",
    "- **Feature Selection Methods**:\n",
    "  - All features\n",
    "  - K-Best with F-classif\n",
    "  - K-Best with mutual information\n",
    "  - L1-based selection\n",
    "  - Tree-based importance\n",
    "- **Scaling Options**:\n",
    "  - No scaling\n",
    "  - Standard scaling\n",
    "  - MinMax scaling\n",
    "  - Robust scaling\n",
    "\n",
    "### 2. Model Types\n",
    "- LightGBM\n",
    "- XGBoost\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "\n",
    "### 3. Hyperparameter Optimization\n",
    "- Uses Hyperopt for Bayesian optimization\n",
    "- Customizable search spaces for each model type\n",
    "- Common parameters across models:\n",
    "  - K-folds (3, 5, 7, 10)\n",
    "  - Split type (time-series/non-time-series)\n",
    "  - Feature selection method\n",
    "  - Number of features\n",
    "  - Scaling method\n",
    "\n",
    "### 4. Evaluation Metrics\n",
    "- AUC-ROC for test and OOT sets\n",
    "- Threshold-based metrics (0.5, 0.8, 0.85, 0.9, 0.95):\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1-score\n",
    "  - AUC\n",
    "- Cross-validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnifiedModelTrainer:\n",
    "    def __init__(self, random_state=2024):\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    ######################\n",
    "    ## Data Preparation ##\n",
    "    ######################\n",
    "    \n",
    "    def prepare_data(self, df, features, target):\n",
    "        \"\"\"Prepare train, validation, test and out-of-time datasets\"\"\"\n",
    "        train_valid = df[df['X_fold'].isin(['train', 'valid'])]\n",
    "        test = df[df['X_fold'] == 'test']\n",
    "        oot = df[df['X_fold'] == 'oot']\n",
    "        \n",
    "        X_train_valid = train_valid[features]\n",
    "        y_train_valid = train_valid[target]\n",
    "        \n",
    "        X_test = test[features]\n",
    "        y_test = test[target]\n",
    "        \n",
    "        X_oot = oot[features]\n",
    "        y_oot = oot[target]\n",
    "        \n",
    "        return (X_train_valid, y_train_valid), (X_test, y_test), (X_oot, y_oot)\n",
    "\n",
    "    ##############################\n",
    "    ## Evaluation Model Metrics ##\n",
    "    ##############################\n",
    "    def evaluate_model(self, model, X_test, y_test, X_oot, y_oot):\n",
    "        \"\"\"Evaluate model performance on test and OOT datasets\"\"\"\n",
    "        if isinstance(model, lgb.Booster) or isinstance(model, RandomForestClassifier):\n",
    "            pred_test = model.predict(X_test)\n",
    "            pred_oot = model.predict(X_oot)\n",
    "    \n",
    "        else:\n",
    "            pred_test = model.predict_proba(X_test)[:, 1]\n",
    "            pred_oot = model.predict_proba(X_oot)[:, 1]\n",
    "        \n",
    "        metrics = {}\n",
    "        metrics['test_auc'] = roc_auc_score(y_test, pred_test)\n",
    "        metrics['oot_auc'] = roc_auc_score(y_oot, pred_oot)\n",
    "        #metrics['recall'] = recall_score(y_test, pred_test)\n",
    "        \n",
    "        # Calculate additional metrics for different thresholds\n",
    "        threshold_metrics = {}\n",
    "        for threshold in [0.5, 0.8, 0.85, 0.9, 0.95]:\n",
    "            pred_test_binary = (pred_test > threshold).astype(int)\n",
    "            pred_oot_binary = (pred_oot > threshold).astype(int)\n",
    "            \n",
    "            threshold_metrics[f'threshold_{threshold}'] = {\n",
    "                'test': {\n",
    "                    'precision': precision_score(y_test, pred_test_binary),\n",
    "                    'recall': recall_score(y_test, pred_test_binary),\n",
    "                    'f1': f1_score(y_test, pred_test_binary),\n",
    "                    'auc': roc_auc_score(y_test, pred_test_binary)\n",
    "                },\n",
    "                'oot': {\n",
    "                    'precision': precision_score(y_oot, pred_oot_binary),\n",
    "                    'recall': recall_score(y_oot, pred_oot_binary),\n",
    "                    'f1': f1_score(y_oot, pred_oot_binary),\n",
    "                    'auc': roc_auc_score(y_oot, pred_oot_binary)\n",
    "\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        metrics['threshold_metrics'] = threshold_metrics\n",
    "        return metrics\n",
    "        \n",
    "    ############################\n",
    "    ## Hyperparameter Tunning ##\n",
    "    ############################\n",
    "    def create_search_space(self, model_type):\n",
    "        \"\"\"Define complete search space including data preparation and model parameters.\"\"\"\n",
    "        \n",
    "        # Common parameters\n",
    "        common_params = {\n",
    "            'k_folds': hp.choice('k_folds', [3, 5, 7, 10]),\n",
    "            'k_split': hp.choice('k_split', ['non_ts', 'ts']),\n",
    "            \n",
    "            # Feature selection parameters\n",
    "            'f_method': hp.choice('f_method', [\n",
    "                'all',  # Use all features\n",
    "                'kbest_f',  # SelectKBest with f_classif\n",
    "                'kbest_mi',  # SelectKBest with mutual_info_classif\n",
    "                'l1',  # L1-based feature selection\n",
    "                'tree_importance'  # Tree-based feature importance\n",
    "            ]),\n",
    "            'num_feats': hp.choice('num_feats', [20, 25, 30, 35, 'all']),\n",
    "            \n",
    "            # Scaling parameters\n",
    "            'scaler': hp.choice('scaler', [\n",
    "                'noscaler',\n",
    "                'standard',\n",
    "                'minmax',\n",
    "                'robust'\n",
    "            ]),\n",
    "            \n",
    "            # Fixed parameters\n",
    "            'SEED': self.random_state\n",
    "        }\n",
    "        \n",
    "        if model_type == 'lgbm':\n",
    "            # LightGBM specific parameters\n",
    "            lgbm_params = {\n",
    "                'num_leaves': hp.quniform('num_leaves', 15, 127, 1),\n",
    "                'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "                'feature_fraction': hp.uniform('feature_fraction', 0.6, 0.9),\n",
    "                'bagging_fraction': hp.uniform('bagging_fraction', 0.6, 0.9),\n",
    "                'bagging_freq': hp.quniform('bagging_freq', 2, 10, 1),\n",
    "                'min_child_samples': hp.quniform('min_child_samples', 10, 150, 1),\n",
    "                'max_depth': hp.quniform('max_depth', 3, 12, 1),\n",
    "                'n_estimators': hp.quniform('n_estimators', 100, 1000, 50)\n",
    "            }\n",
    "            return {**common_params, **lgbm_params}\n",
    "\n",
    "        elif model_type == 'xgb':\n",
    "            # XGBoost specific parameters\n",
    "            xgb_params = {\n",
    "                'max_depth': hp.quniform('max_depth', 3, 12, 1),\n",
    "                'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "                'subsample': hp.uniform('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
    "                'min_child_weight': hp.quniform('min_child_weight', 1, 7, 1),\n",
    "                'n_estimators': hp.quniform('n_estimators', 100, 1000, 50)\n",
    "            }\n",
    "            return {**common_params, **xgb_params}\n",
    "        \n",
    "        elif model_type == 'rf':\n",
    "            rf_params = {\n",
    "                'n_estimators': hp.quniform('n_estimators', 100, 1000, 50),\n",
    "                'max_depth': hp.quniform('max_depth', 3, 12, 1),\n",
    "                'min_samples_split': hp.quniform('min_samples_split', 2, 10, 1),\n",
    "                'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 5, 1)\n",
    "            }\n",
    "            return {**common_params, **rf_params}\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid model type. Please choose 'lgbm' or 'xgb'.\")\n",
    "\n",
    "    \n",
    "    def apply_feature_selection(self, X, y, method, num_feats):\n",
    "        \"\"\"Apply feature selection based on specified method\"\"\"\n",
    "        if method == 'all' or num_feats == 'all':\n",
    "            return X\n",
    "            \n",
    "        n_features = min(num_feats, X.shape[1])\n",
    "        \n",
    "        if method == 'kbest_f':\n",
    "            selector = SelectKBest(score_func=f_classif, k=n_features)\n",
    "        elif method == 'kbest_mi':\n",
    "            selector = SelectKBest(score_func=mutual_info_classif, k=n_features)\n",
    "        elif method == 'l1':\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            selector = LogisticRegression(penalty='l1', solver='saga', random_state=self.random_state)\n",
    "            selector.fit(X, y)\n",
    "            mask = np.abs(selector.coef_[0]) > 0\n",
    "            return X.iloc[:, mask]\n",
    "        elif method == 'tree_importance':\n",
    "            model = lgb.LGBMClassifier(random_state=self.random_state)\n",
    "            model.fit(X, y)\n",
    "            importance = pd.Series(model.feature_importances_, index=X.columns)\n",
    "            selected_features = importance.nlargest(n_features).index\n",
    "            return X[selected_features]\n",
    "            \n",
    "        if method in ['kbest_f', 'kbest_mi']:\n",
    "            selector.fit(X, y)\n",
    "            mask = selector.get_support()\n",
    "            return X.iloc[:, mask]\n",
    "            \n",
    "        return X\n",
    "    \n",
    "    def plot_results(self, metrics, output_dir):\n",
    "        \"\"\"Create and save visualization of results\"\"\"\n",
    "        # Plot threshold performance\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        thresholds = sorted(metrics['threshold_metrics'].keys(), \n",
    "                          key=lambda x: float(x.split('_')[1]))\n",
    "        \n",
    "        test_precision = [metrics['threshold_metrics'][t]['test']['precision'] \n",
    "                         for t in thresholds]\n",
    "        oot_precision = [metrics['threshold_metrics'][t]['oot']['precision'] \n",
    "                        for t in thresholds]\n",
    "        \n",
    "        plt.plot(thresholds, test_precision, label='Test Precision')\n",
    "        plt.plot(thresholds, oot_precision, label='OOT Precision')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Model Performance Across Thresholds')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/threshold_performance.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def apply_scaling(self, X_train, X_test, X_oot, scaler_type):\n",
    "        \"\"\"Apply scaling transformation to the data\"\"\"\n",
    "        if scaler_type == 'noscaler':\n",
    "            return X_train, X_test, X_oot\n",
    "            \n",
    "        if scaler_type == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaler_type == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaler_type == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "            \n",
    "        X_train_scaled = pd.DataFrame(\n",
    "            scaler.fit_transform(X_train),\n",
    "            columns=X_train.columns,\n",
    "            index=X_train.index\n",
    "        )\n",
    "        X_test_scaled = pd.DataFrame(\n",
    "            scaler.transform(X_test),\n",
    "            columns=X_test.columns,\n",
    "            index=X_test.index\n",
    "        )\n",
    "        X_oot_scaled = pd.DataFrame(\n",
    "            scaler.transform(X_oot),\n",
    "            columns=X_oot.columns,\n",
    "            index=X_oot.index\n",
    "        )\n",
    "        \n",
    "        return X_train_scaled, X_test_scaled, X_oot_scaled\n",
    "    \n",
    "    def get_cv_splitter(self, k_split, k_folds):\n",
    "        \"\"\"Get cross-validation splitter based on specified method\"\"\"\n",
    "        if k_split == 'non_ts':\n",
    "            return KFold(n_splits=k_folds, shuffle=True, random_state=self.random_state)\n",
    "        else:\n",
    "            return TimeSeriesSplit(n_splits=k_folds)\n",
    "\n",
    "###########################################\n",
    "## Objective Functions for LGBM, XGB, RF ##\n",
    "###########################################\n",
    "\n",
    "    def objective_lgb(self, space, X_train, y_train, X_test, y_test, X_oot, y_oot):\n",
    "        \"\"\"Enhanced objective function for LightGBM optimization\"\"\"\n",
    "        try:\n",
    "            # Apply feature selection\n",
    "            X_train_selected = self.apply_feature_selection(\n",
    "                X_train, y_train, space['f_method'], space['num_feats']\n",
    "            )\n",
    "            X_test_selected = X_test[X_train_selected.columns]\n",
    "            X_oot_selected = X_oot[X_train_selected.columns]\n",
    "            \n",
    "            # Apply scaling\n",
    "            X_train_processed, X_test_processed, X_oot_processed = self.apply_scaling(\n",
    "                X_train_selected, X_test_selected, X_oot_selected, space['scaler']\n",
    "            )\n",
    "            \n",
    "            # Create CV splitter\n",
    "            cv_splitter = self.get_cv_splitter(space['k_split'], int(space['k_folds']))\n",
    "            \n",
    "            # Prepare LightGBM parameters\n",
    "            params = {\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',#auc\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': int(space['num_leaves']),\n",
    "                'learning_rate': space['learning_rate'],\n",
    "                'feature_fraction': space['feature_fraction'],\n",
    "                'bagging_fraction': space['bagging_fraction'],\n",
    "                'bagging_freq': int(space['bagging_freq']),\n",
    "                'min_child_samples': int(space['min_child_samples']),\n",
    "                'max_depth': int(space['max_depth']),\n",
    "                'n_estimators': int(space['n_estimators']),\n",
    "                'verbose': -1,\n",
    "                'random_state': space['SEED']\n",
    "            }\n",
    "            \n",
    "            # Cross-validation scores\n",
    "            cv_scores = []\n",
    "            for train_idx, valid_idx in cv_splitter.split(X_train_processed):\n",
    "                X_fold_train = X_train_processed.iloc[train_idx]\n",
    "                y_fold_train = y_train.iloc[train_idx]\n",
    "                X_fold_valid = X_train_processed.iloc[valid_idx]\n",
    "                y_fold_valid = y_train.iloc[valid_idx]\n",
    "                \n",
    "                train_data = lgb.Dataset(X_fold_train, label=y_fold_train)\n",
    "                valid_data = lgb.Dataset(X_fold_valid, label=y_fold_valid, reference=train_data)\n",
    "                \n",
    "                model = lgb.train(\n",
    "                    params,\n",
    "                    train_data,\n",
    "                    valid_sets=[valid_data],\n",
    "                    callbacks=[lgb.early_stopping(stopping_rounds=20)]\n",
    "                )\n",
    "                \n",
    "                pred_valid = model.predict(X_fold_valid)\n",
    "                cv_scores.append(roc_auc_score(y_fold_valid, pred_valid)) # modify metric here for tuning for other metrics\n",
    "            \n",
    "            # Train final model on full training data\n",
    "            train_data = lgb.Dataset(X_train_processed, label=y_train)\n",
    "            valid_data = lgb.Dataset(X_test_processed, label=y_test, reference=train_data)\n",
    "            \n",
    "            final_model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                valid_sets=[valid_data],\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=20)]\n",
    "            )\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.evaluate_model(\n",
    "                final_model, \n",
    "                X_test_processed, y_test,\n",
    "                X_oot_processed, y_oot\n",
    "            )\n",
    "            \n",
    "            metrics['cv_score_mean'] = np.mean(cv_scores)\n",
    "            metrics['cv_score_std'] = np.std(cv_scores)\n",
    "            \n",
    "            return {\n",
    "                'loss': -metrics['cv_score_mean'],  # Optimize for CV performance\n",
    "                'status': STATUS_OK,\n",
    "                'model': final_model,\n",
    "                'metrics': metrics,\n",
    "                'params': params,\n",
    "                'feature_columns': list(X_train_processed.columns)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in objective function: {str(e)}\")\n",
    "            return {'loss': 0, 'status': STATUS_OK, 'model': None}\n",
    "    \n",
    "    def objective_xgb(self, space, X_train, y_train, X_test, y_test, X_oot, y_oot):\n",
    "        \"\"\"Enhanced objective function for XGBoost optimization\"\"\"\n",
    "        try:\n",
    "            # Apply feature selection\n",
    "            X_train_selected = self.apply_feature_selection(\n",
    "                X_train, y_train, space['f_method'], space['num_feats']\n",
    "            )\n",
    "            X_test_selected = X_test[X_train_selected.columns]\n",
    "            X_oot_selected = X_oot[X_train_selected.columns]\n",
    "            \n",
    "            # Apply scaling\n",
    "            X_train_processed, X_test_processed, X_oot_processed = self.apply_scaling(\n",
    "                X_train_selected, X_test_selected, X_oot_selected, space['scaler']\n",
    "            )\n",
    "            \n",
    "            # Create CV splitter\n",
    "            cv_splitter = self.get_cv_splitter(space['k_split'], int(space['k_folds']))\n",
    "            \n",
    "            # Prepare XGBoost parameters\n",
    "            params = {\n",
    "                'objective': 'binary:logistic',\n",
    "                'eval_metric': 'auc',\n",
    "                'max_depth': int(space['max_depth']),\n",
    "                'learning_rate': space['learning_rate'],\n",
    "                'subsample': space['subsample'],\n",
    "                'colsample_bytree': space['colsample_bytree'],\n",
    "                'min_child_weight': int(space['min_child_weight']),\n",
    "                'n_estimators': int(space['n_estimators']),\n",
    "                'random_state': space['SEED']\n",
    "            }\n",
    "            \n",
    "            # Cross-validation scores\n",
    "            cv_scores = []\n",
    "            for train_idx, valid_idx in cv_splitter.split(X_train_processed):\n",
    "                X_fold_train = X_train_processed.iloc[train_idx]\n",
    "                y_fold_train = y_train.iloc[train_idx]\n",
    "                X_fold_valid = X_train_processed.iloc[valid_idx]\n",
    "                y_fold_valid = y_train.iloc[valid_idx]\n",
    "                \n",
    "                model = xgb.XGBClassifier(**params)\n",
    "                model.fit(X_fold_train, y_fold_train, eval_set=[(X_fold_valid, y_fold_valid)], \n",
    "                        verbose=False)\n",
    "                \n",
    "                pred_valid = model.predict_proba(X_fold_valid)[:, 1]\n",
    "                cv_scores.append(roc_auc_score(y_fold_valid, pred_valid))\n",
    "            \n",
    "            # Train final model on full training data\n",
    "            model = xgb.XGBClassifier(**params)\n",
    "            model.fit(X_train_processed, y_train, eval_set=[(X_test_processed, y_test)], \n",
    "                     verbose=False)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = self.evaluate_model(\n",
    "                model, \n",
    "                X_test_processed, y_test,\n",
    "                X_oot_processed, y_oot\n",
    "            )\n",
    "            \n",
    "            metrics['cv_score_mean'] = np.mean(cv_scores)\n",
    "            metrics['cv_score_std'] = np.std(cv_scores)\n",
    "            \n",
    "            return {\n",
    "                'loss': -metrics['cv_score_mean'],  # Optimize for CV performance\n",
    "                'status': STATUS_OK,\n",
    "                'model': model,\n",
    "                'metrics': metrics,\n",
    "                'params': params,\n",
    "                'feature_columns': list(X_train_processed.columns)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in objective function: {str(e)}\")\n",
    "            return {'loss': 0, 'status': STATUS_OK, 'model': None}\n",
    "\n",
    "    def objective_rf(self, space, X_train, y_train, X_test, y_test, X_oot, y_oot):\n",
    "        \"\"\"Objective function for RandomForestClassifier\"\"\"\n",
    "        try:\n",
    "            # Feature selection and scaling\n",
    "            X_train_selected = self.apply_feature_selection(X_train, y_train, space['f_method'], space['num_feats'])\n",
    "            X_test_selected = X_test[X_train_selected.columns]\n",
    "            X_oot_selected = X_oot[X_train_selected.columns]\n",
    "            X_train_scaled, X_test_scaled, X_oot_scaled = self.apply_scaling(X_train_selected, X_test_selected, X_oot_selected, space['scaler'])\n",
    "\n",
    "            # Set parameters for RandomForestClassifier\n",
    "            params = {\n",
    "                'n_estimators': int(space['n_estimators']),\n",
    "                'max_depth': int(space['max_depth']),\n",
    "                'min_samples_split': int(space['min_samples_split']),\n",
    "                'min_samples_leaf': int(space['min_samples_leaf']),\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "            \n",
    "            model = RandomForestClassifier(**params)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            metrics = self.evaluate_model(model, X_test_scaled, y_test, X_oot_scaled, y_oot)\n",
    "            \n",
    "            return {'loss': -metrics['test_auc'], \n",
    "                    'model': model, \n",
    "                    'status': STATUS_OK, \n",
    "                    'metrics': metrics,\n",
    "                    'params': params,\n",
    "                    'feature_columns': list(X_train_selected.columns)}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"RandomForest Error: {str(e)}\")\n",
    "            return {'loss': 0, 'status': STATUS_OK, 'model': None}\n",
    "    \n",
    "    #########################\n",
    "    ## Logistic Regression ##\n",
    "    #########################\n",
    "    def run_logistic_regression(self, df, features, target):\n",
    "\n",
    "        # Prepare the data using the prepare_data method\n",
    "        (X_train_valid, y_train_valid), (X_test, y_test), (X_oot, y_oot) = self.prepare_data(df, features, target)\n",
    "        \n",
    "        # Initialize and fit the logistic regression model\n",
    "        log_reg = LogisticRegression()\n",
    "        log_reg.fit(X_train_valid, y_train_valid)\n",
    "        \n",
    "        # Make predictions on the test set\n",
    "        y_pred_prob = log_reg.predict_proba(X_test)[:,1]\n",
    "        y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "        # Calculate accuracy, confusion matrix, and classification report\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        print(\"Classification Report:\")\n",
    "        print(class_report)\n",
    "        \n",
    "        # Store metrics in a dictionary for further use\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"confusion_matrix\": conf_matrix,\n",
    "            \"classification_report\": class_report\n",
    "        }\n",
    "        \n",
    "        return log_reg, metrics\n",
    "\n",
    "    def optimize_model(self, df, features, target, max_evals=20, model_type = \"\"):\n",
    "        \"\"\"Run complete model optimization pipeline with enhanced search space\"\"\"\n",
    "        # Prepare data\n",
    "        (X_train_valid, y_train_valid), (X_test, y_test), (X_oot, y_oot) = self.prepare_data(\n",
    "            df, features, target\n",
    "        )\n",
    "        if model_type == 'lgbm':\n",
    "        # Define objective function with prepared data\n",
    "            objective = partial(\n",
    "                self.objective_lgb,\n",
    "                X_train=X_train_valid,\n",
    "                y_train=y_train_valid,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                X_oot=X_oot,\n",
    "                y_oot=y_oot\n",
    "            )\n",
    "        \n",
    "        if model_type == 'xgb':\n",
    "            objective = partial(\n",
    "                self.objective_xgb,\n",
    "                X_train=X_train_valid,\n",
    "                y_train=y_train_valid,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                X_oot=X_oot,\n",
    "                y_oot=y_oot\n",
    "            )\n",
    "        \n",
    "        if model_type == 'rf':\n",
    "            objective = partial(\n",
    "                self.objective_rf,\n",
    "                X_train=X_train_valid,\n",
    "                y_train=y_train_valid,\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                X_oot=X_oot,\n",
    "                y_oot=y_oot\n",
    "            )\n",
    "        \n",
    "        # Run optimization\n",
    "        trials = Trials()\n",
    "        best = fmin(\n",
    "            fn=objective,\n",
    "            space=self.create_search_space(model_type),\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=max_evals,\n",
    "            trials=trials\n",
    "        )\n",
    "        \n",
    "        # Get best trial\n",
    "        best_trial = sorted(trials.trials, key=lambda x: x['result']['loss'])[0]\n",
    "        \n",
    "        # Store trials history\n",
    "        self.trials_history = pd.DataFrame([\n",
    "            {\n",
    "                **trial['misc']['vals'],\n",
    "                'cv_score': -trial['result']['loss'],\n",
    "                'test_auc': trial['result']['metrics']['test_auc'],\n",
    "                'oot_auc': trial['result']['metrics']['oot_auc'],\n",
    "                **{\n",
    "                    f\"threshold_{threshold}_{metric}_{set_type}\": trial['result']['metrics']['threshold_metrics'][f'threshold_{threshold}'][set_type][metric]\n",
    "                    for threshold in [0.5,0.8, 0.85, 0.9, 0.95]\n",
    "                    for set_type in ['test', 'oot']\n",
    "                    for metric in ['precision', 'recall', 'f1', 'auc']\n",
    "                }\n",
    "            }\n",
    "            for trial in trials.trials\n",
    "            if 'loss' in trial['result']\n",
    "        ])\n",
    "        \n",
    "        return (\n",
    "            \n",
    "            best_trial['result']['model'],\n",
    "            best_trial['result']['metrics'],\n",
    "            best_trial['result']['params'],\n",
    "            best_trial['result']['feature_columns']\n",
    "            \n",
    "         )\n",
    "    \n",
    "    ###################\n",
    "    ## Replicability ##\n",
    "    ###################\n",
    "    def save_model(self, model, output_dir, model_type, model_name='_best_model.pkl'):\n",
    "        \"\"\"Save the trained XGBoost model to a pickle file.\"\"\"\n",
    "        with open(f\"{output_dir}/{model_type + model_name}\", 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        \"\"\"Load a trained XGBoost model from a pickle file.\"\"\"\n",
    "        with open(model_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segregation of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feature Columns and Target\n",
    "id_col = ['id']\n",
    "labels = ['account_type']\n",
    "\n",
    "predictive_cols = ['default_profile', 'default_profile_image',\n",
    "       'favourites_count', 'followers_count', 'friends_count', 'geo_enabled',\n",
    "        'lang', 'location', 'statuses_count', 'verified',\n",
    "       'average_tweets_per_day', 'account_age_days', \n",
    "       'is_description_na', 'is_lang_na', 'is_lang_en', 'deviation_from_humans',\n",
    "       'is_location_unknown', 'creation_hour', 'creation_day_of_week',\n",
    "       'creation_month', 'creation_year', 'is_weekend', 'creation_quarter',\n",
    "       'part_of_day', 'creation_week_of_year', 'is_beginning_of_month',\n",
    "       'is_end_of_month', 'description_length', 'influencer_type',\n",
    "       'favourites_per_day', 'favourites_activity',\n",
    "       'followers_to_friends_ratio', 'followers_to_tweets_per_day_ratio',\n",
    "       'friends_to_tweets_per_day_ratio', 'mention_count',\n",
    "       'log_favourites_count', 'log_followers_count', 'log_friends_count',\n",
    "       'log_statuses_count', 'log_average_tweets_per_day',\n",
    "       'log_fol_to_friends_ratio', 'log_fol_to_tweets_ratio',\n",
    "       'log_friends_to_tweets_ratio','account_type']\n",
    "\n",
    "non_predictive_cols = ['description','profile_background_image_url','profile_image_url','screen_name','mentions']\n",
    "\n",
    "target = ['account_type']\n",
    "\n",
    "id = ['id']\n",
    "\n",
    "predictive_cols_log = ['default_profile', 'default_profile_image',\n",
    "        'geo_enabled', 'deviation_from_humans', 'location', 'verified',\n",
    "        'account_age_days', 'is_description_na', 'is_lang_na', 'is_lang_en',\n",
    "       'is_location_unknown', 'creation_hour', 'creation_day_of_week',\n",
    "       'creation_month', 'creation_year', 'is_weekend', 'creation_quarter',\n",
    "       'part_of_day', 'creation_week_of_year', 'is_beginning_of_month',\n",
    "       'is_end_of_month', 'description_length', 'influencer_type',\n",
    "       'favourites_per_day', 'favourites_activity',\n",
    "       'mention_count', 'log_favourites_count', 'log_followers_count', 'log_friends_count',\n",
    "       'log_statuses_count', 'log_average_tweets_per_day',\n",
    "       'log_fol_to_friends_ratio', 'log_fol_to_tweets_ratio',\n",
    "       'log_friends_to_tweets_ratio','account_type']\n",
    "\n",
    "predictive_cols_nolog = ['default_profile', 'default_profile_image',\n",
    "       'favourites_count', 'followers_count', 'friends_count', 'geo_enabled',\n",
    "       'deviation_from_humans', 'lang', 'location', 'statuses_count', 'verified',\n",
    "       'average_tweets_per_day', 'account_age_days', \n",
    "       'is_description_na', 'is_lang_na', 'is_lang_en',\n",
    "       'is_location_unknown', 'creation_hour', 'creation_day_of_week',\n",
    "       'creation_month', 'creation_year', 'is_weekend', 'creation_quarter',\n",
    "       'part_of_day', 'creation_week_of_year', 'is_beginning_of_month',\n",
    "       'is_end_of_month', 'description_length', 'influencer_type',\n",
    "       'favourites_per_day', 'favourites_activity',\n",
    "       'followers_to_friends_ratio', 'followers_to_tweets_per_day_ratio',\n",
    "       'friends_to_tweets_per_day_ratio', 'mention_count','account_type']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Log Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n",
      "Confusion Matrix:\n",
      "[[4266  445]\n",
      " [ 732 1383]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88      4711\n",
      "           1       0.76      0.65      0.70      2115\n",
      "\n",
      "    accuracy                           0.83      6826\n",
      "   macro avg       0.81      0.78      0.79      6826\n",
      "weighted avg       0.82      0.83      0.82      6826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = UnifiedModelTrainer(random_state=2024)\n",
    "dftmp = df\n",
    "\n",
    "featureslog = predictive_cols_log\n",
    "\n",
    "log_reg, metrics = trainer.run_logistic_regression(dftmp, featureslog, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with non-Log Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74\n",
      "Confusion Matrix:\n",
      "[[4265  446]\n",
      " [1351  764]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.91      0.83      4711\n",
      "           1       0.63      0.36      0.46      2115\n",
      "\n",
      "    accuracy                           0.74      6826\n",
      "   macro avg       0.70      0.63      0.64      6826\n",
      "weighted avg       0.72      0.74      0.71      6826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = UnifiedModelTrainer(random_state=2024)\n",
    "dftmp = df\n",
    "featuresnolog = predictive_cols_nolog\n",
    "\n",
    "log_reg, metrics = trainer.run_logistic_regression(dftmp, featuresnolog, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Tree Based Models\n",
    "\n",
    "- Random Forest\n",
    "- XGBoost\n",
    "- LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8658, number of negative: 18645           \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3395                                               \n",
      "[LightGBM] [Info] Number of data points in the train set: 27303, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.317108 -> initscore=-0.767094 \n",
      "[LightGBM] [Info] Start training from score -0.767094                           \n",
      "100%|██████████| 10/10 [02:40<00:00, 16.03s/trial, best loss: -0.8430810040180594]\n",
      "[LightGBM] [Info] Number of positive: 8658, number of negative: 18645            \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3395                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 27303, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.317108 -> initscore=-0.767094  \n",
      "[LightGBM] [Info] Start training from score -0.767094                            \n",
      "[LightGBM] [Info] Number of positive: 8658, number of negative: 18645            \n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3395                                                \n",
      "[LightGBM] [Info] Number of data points in the train set: 27303, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.317108 -> initscore=-0.767094  \n",
      "[LightGBM] [Info] Start training from score -0.767094                            \n",
      "100%|██████████| 10/10 [01:47<00:00, 10.80s/trial, best loss: -0.9371896577240829]\n",
      "[LightGBM] [Info] Number of positive: 8658, number of negative: 18645\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001322 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3395                     \n",
      "[LightGBM] [Info] Number of data points in the train set: 27303, number of used features: 33\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.317108 -> initscore=-0.767094\n",
      "[LightGBM] [Info] Start training from score -0.767094 \n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:       \n",
      "[700]\tvalid_0's auc: 0.936183\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:       \n",
      "[700]\tvalid_0's auc: 0.931169\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:       \n",
      "[699]\tvalid_0's auc: 0.933958\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:       \n",
      "[699]\tvalid_0's auc: 0.933358\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:       \n",
      "[696]\tvalid_0's auc: 0.939539\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:       \n",
      "[699]\tvalid_0's auc: 0.939077\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[39]\tvalid_0's auc: 0.927265\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[36]\tvalid_0's auc: 0.934606\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[52]\tvalid_0's auc: 0.93404\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[60]\tvalid_0's auc: 0.93128\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[69]\tvalid_0's auc: 0.929467\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[95]\tvalid_0's auc: 0.941486\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[149]\tvalid_0's auc: 0.937865\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[149]\tvalid_0's auc: 0.932745\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[150]\tvalid_0's auc: 0.9335\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[150]\tvalid_0's auc: 0.936838\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[150]\tvalid_0's auc: 0.929824\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[150]\tvalid_0's auc: 0.941801\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[150]\tvalid_0's auc: 0.941687\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[150]\tvalid_0's auc: 0.941084\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[150]\tvalid_0's auc: 0.938857\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[168]\tvalid_0's auc: 0.93596\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[202]\tvalid_0's auc: 0.934484\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[236]\tvalid_0's auc: 0.937441\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[215]\tvalid_0's auc: 0.931645\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[133]\tvalid_0's auc: 0.941899\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[186]\tvalid_0's auc: 0.941467\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[216]\tvalid_0's auc: 0.940704\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[298]\tvalid_0's auc: 0.918983\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[290]\tvalid_0's auc: 0.922859\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[290]\tvalid_0's auc: 0.930113\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[289]\tvalid_0's auc: 0.933567\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[374]\tvalid_0's auc: 0.939392\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[425]\tvalid_0's auc: 0.923854\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[409]\tvalid_0's auc: 0.933439\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[260]\tvalid_0's auc: 0.934676\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[477]\tvalid_0's auc: 0.930735\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[499]\tvalid_0's auc: 0.92806\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[494]\tvalid_0's auc: 0.940139\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[283]\tvalid_0's auc: 0.93447\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[585]\tvalid_0's auc: 0.933852\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[539]\tvalid_0's auc: 0.93079\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[388]\tvalid_0's auc: 0.930326\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[345]\tvalid_0's auc: 0.939037\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[418]\tvalid_0's auc: 0.927774\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[384]\tvalid_0's auc: 0.928335\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[459]\tvalid_0's auc: 0.934893\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[396]\tvalid_0's auc: 0.936391\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[215]\tvalid_0's auc: 0.939219\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[529]\tvalid_0's auc: 0.937613\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[59]\tvalid_0's auc: 0.939215\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[48]\tvalid_0's auc: 0.934347\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[69]\tvalid_0's auc: 0.933147\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[95]\tvalid_0's auc: 0.935651\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[44]\tvalid_0's auc: 0.929926\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[43]\tvalid_0's auc: 0.942015\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[45]\tvalid_0's auc: 0.939968\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[48]\tvalid_0's auc: 0.939767\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[399]\tvalid_0's auc: 0.93301\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[616]\tvalid_0's auc: 0.934517\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[455]\tvalid_0's auc: 0.929496\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[521]\tvalid_0's auc: 0.94155\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[239]\tvalid_0's auc: 0.923568\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[206]\tvalid_0's auc: 0.929538\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[283]\tvalid_0's auc: 0.935432\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[295]\tvalid_0's auc: 0.929739\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[334]\tvalid_0's auc: 0.935536\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[306]\tvalid_0's auc: 0.933754\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[320]\tvalid_0's auc: 0.92624\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[316]\tvalid_0's auc: 0.940569\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[179]\tvalid_0's auc: 0.93094\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[198]\tvalid_0's auc: 0.932687\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[345]\tvalid_0's auc: 0.938379\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[347]\tvalid_0's auc: 0.930219\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[348]\tvalid_0's auc: 0.936149\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Early stopping, best iteration is:                                               \n",
      "[286]\tvalid_0's auc: 0.932471\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[446]\tvalid_0's auc: 0.930716\n",
      "Training until validation scores don't improve for 20 rounds                     \n",
      "Did not meet early stopping. Best iteration is:                                  \n",
      "[450]\tvalid_0's auc: 0.941936\n",
      "100%|██████████| 10/10 [01:29<00:00,  8.90s/trial, best loss: -0.9373933551079698]\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = UnifiedModelTrainer(random_state=2024)\n",
    "\n",
    "features = [col for col in predictive_cols_log if col not in ['id', 'account_type', 'X_fold']]\n",
    "\n",
    "target = 'account_type'\n",
    "\n",
    "# Run optimization with extended search space\n",
    "modelrf, metricsrf, paramsrf, selected_featuresrf = trainer.optimize_model(\n",
    "    df, features, target, max_evals=10, model_type = 'rf'\n",
    ")\n",
    "modelxgb, metricsxgb, paramsxgb, selected_featuresxgb = trainer.optimize_model(\n",
    "    df, features, target, max_evals=10, model_type = 'xgb'\n",
    ")\n",
    "\n",
    "modellgbm, metricslgbm, paramslgbm, selected_featureslgbm = trainer.optimize_model(\n",
    "    df, features, target, max_evals=10, model_type = 'lgbm'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicability\n",
    "\n",
    "- Save Models\n",
    "- Save performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Models\n",
    "trainer.save_model(modelxgb, model_type = 'xgb', output_dir= \"output_files\")\n",
    "trainer.save_model(modelrf, model_type = 'rf', output_dir= \"output_files\")\n",
    "trainer.save_model(modellgbm, model_type = 'lgbm', output_dir= \"output_files\")\n",
    "\n",
    "# Saving performance metrics\n",
    "data = [\n",
    "    {\n",
    "        'model': 'RandomForest',\n",
    "        'metrics': metricsrf,\n",
    "        'parameters': paramsrf,\n",
    "        'selected_features': selected_featuresrf\n",
    "    },\n",
    "    {\n",
    "        'model': 'XGBoost',\n",
    "        'metrics': metricsxgb,\n",
    "        'parameters': paramsxgb,\n",
    "        'selected_features': selected_featuresxgb\n",
    "    },\n",
    "    {\n",
    "        'model': 'LightGBM',\n",
    "        'metrics': metricslgbm,\n",
    "        'parameters': paramslgbm,\n",
    "        'selected_features': selected_featureslgbm\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_results = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df_results.to_csv('output_files/model_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
