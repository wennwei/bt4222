{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "from ftfy import fix_text\n",
    "from langdetect import detect, LangDetectException\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn.functional as F\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, \n",
    "                             confusion_matrix, classification_report)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# Ignore Warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# Check cuda status\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV files\n",
    "df = pd.read_csv('source data/scraped_tweets.csv')\n",
    "df_main = pd.read_csv(\"cleaned data/cleaned_data_split.csv\", index_col=0)\n",
    "df_main['Handle'] = '@' + df_main['screen_name']\n",
    "\n",
    "# Merge datasets on Twitter handle\n",
    "result = df.merge(df_main, on='Handle', how='inner')\n",
    "\n",
    "# Remove duplicates based on unique identifier 'Handle'\n",
    "result = result.drop_duplicates(subset=['Handle'])\n",
    "\n",
    "# Replace NaN values in specific columns with 'NA'\n",
    "result.fillna({'Content': 'NA', 'description': 'NA'}, inplace=True)\n",
    "\n",
    "# Fix text encoding issues in 'Content' and 'description' columns\n",
    "result['Content'] = result['Content'].apply(fix_text)\n",
    "result['description'] = result['description'].apply(fix_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and filtered data (English-only for 'Content') has been saved as 'cleaned_data_content_english_only.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Define language detection function to filter English-only content\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False  # Mark as non-English if detection fails\n",
    "\n",
    "# Filter rows where 'Content' is in English only\n",
    "result_final = result[result['Content'].apply(is_english)]\n",
    "\n",
    "# Optional: Save cleaned and filtered data to a new CSV file\n",
    "# result_final.to_csv(\"cleaned_data_content_english_only.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Cleaned and filtered data (English-only for 'Content') has been saved as 'cleaned_data_content_english_only.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_7048\\1234352611.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_final['cleaned_content'] = result_final['Content'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "# Text cleaning for Content column\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)  # Remove mentions and hashtags\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    text = text.lower()  # Convert to lowercase for uncased model\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Optionally remove punctuation\n",
    "    return text\n",
    "\n",
    "result_final['cleaned_content'] = result_final['Content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Stratified Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained BERT tokenizer for processing input text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Convert cleaned DataFrame to Hugging Face Dataset format\n",
    "result_final = result_final[['cleaned_content', 'account_type']].reset_index(drop=True)\n",
    "dataset = Dataset.from_pandas(result_final)\n",
    "\n",
    "# Convert Hugging Face Dataset back to pandas DataFrame for stratified split usage\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Separate features and target label\n",
    "X = df.drop(columns=[\"account_type\"])\n",
    "y = df[\"account_type\"]\n",
    "\n",
    "# Perform first split: 60% training and 40% temporary set for further division\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=42)\n",
    "train_index, temp_index = next(sss.split(X, y))\n",
    "train_df = df.iloc[train_index]\n",
    "temp_df = df.iloc[temp_index]\n",
    "\n",
    "# Split temporary set: 50% validation and 50% test\n",
    "sss_temp = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_index, test_index = next(sss_temp.split(temp_df.drop(columns=[\"account_type\"]), temp_df[\"account_type\"]))\n",
    "val_df = temp_df.iloc[val_index]\n",
    "test_df = temp_df.iloc[test_index]\n",
    "\n",
    "# Convert DataFrames back to Hugging Face Dataset format for compatibility with transformers\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of content is:  57\n"
     ]
    }
   ],
   "source": [
    "# Check max content length for padding\n",
    "content_length = result_final['cleaned_content'].apply(lambda x: len(str(x).split()))\n",
    "max_content_len = max(content_length)\n",
    "\n",
    "print(\"Max length of content is: \", max_content_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a377fc900ea4dd9abaaf589b5951e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1093 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6489e3c3c64059a64faa21225a3acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a7a758d8394cd7ab8fd69fbd99a49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/365 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1093\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 364\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 365\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"cleaned_content\"], padding=\"max_length\", truncation=True, max_length=max_content_len)\n",
    "\n",
    "# Apply tokenization to training, validation, and test datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Rename 'account_type' column to 'labels' to align with model's expected input format\n",
    "tokenized_train = tokenized_train.rename_column(\"account_type\", \"labels\")\n",
    "tokenized_val = tokenized_val.rename_column(\"account_type\", \"labels\")\n",
    "tokenized_test = tokenized_test.rename_column(\"account_type\", \"labels\")\n",
    "\n",
    "# Remove unnecessary columns (e.g., 'cleaned_content') from datasets\n",
    "tokenized_train = tokenized_train.remove_columns([\"cleaned_content\"])\n",
    "tokenized_val = tokenized_val.remove_columns([\"cleaned_content\"])\n",
    "tokenized_test = tokenized_test.remove_columns([\"cleaned_content\"])\n",
    "\n",
    "# Set data format for PyTorch model input with 'input_ids', 'attention_mask', and 'labels'\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Combine datasets into a DatasetDict for easy access during training and evaluation\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": tokenized_train,\n",
    "    \"validation\": tokenized_val,\n",
    "    \"test\": tokenized_test\n",
    "})\n",
    "\n",
    "# Display structure of the final DatasetDict to confirm format\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size for data loading\n",
    "batch_size = 25\n",
    "\n",
    "# Create DataLoaders for training, validation, and test sets\n",
    "# DataLoaders handle batching and shuffling for efficient model training and evaluation\n",
    "train_dataloader = DataLoader(tokenized_train, shuffle=True, batch_size=batch_size)  # Shuffle for training\n",
    "val_dataloader = DataLoader(tokenized_val, batch_size=batch_size)  # No shuffle for validation\n",
    "test_dataloader = DataLoader(tokenized_test, batch_size=batch_size)  # No shuffle for testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, output_dim, dropout_rate):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # Initialize BERT encoder with output hidden states for further processing\n",
    "        self.encoder = AutoModelForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\", output_hidden_states=True, return_dict=True\n",
    "        )\n",
    "        \n",
    "        # Additional layers: dropout, layer normalization, and custom classifier\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(3072)  # Normalization layer\n",
    "        self.classifier = nn.Linear(3072, output_dim)  # Classification layer\n",
    "\n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        # Perform mean pooling on token embeddings, accounting for attention mask\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        # Forward pass through BERT encoder to get hidden states\n",
    "        outputs = self.encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Concatenate the last four hidden layers for enhanced contextual representation\n",
    "        hidden_states = torch.cat([outputs.hidden_states[i] for i in [-1, -2, -3, -4]], dim=-1)\n",
    "\n",
    "        # Apply layer normalization for stable training\n",
    "        hidden_states = self.layer_norm(hidden_states)\n",
    "\n",
    "        # Use mean pooling to aggregate token embeddings\n",
    "        pooled_output = self.mean_pooling(hidden_states, attention_mask)\n",
    "\n",
    "        # Pass through dropout and then classification layer\n",
    "        x = self.dropout(pooled_output)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the custom model with specified output dimensions and dropout rate\n",
    "model = Model(output_dim=2, dropout_rate=0.5)\n",
    "\n",
    "# Set device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to the appropriate device\n",
    "\n",
    "# Calculate class weights to address potential class imbalance\n",
    "labels = result_final['account_type'].values\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Define custom Focal Loss class to handle class imbalance by emphasizing hard-to-classify samples\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, class_weights=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # Scaling factor for loss\n",
    "        self.gamma = gamma  # Modulates the effect of easy vs. hard samples\n",
    "        self.class_weights = class_weights  # Optional weights for class imbalance\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Compute Cross-Entropy Loss, weighted by class weights\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, weight=self.class_weights, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)  # Model's predicted probability for true class\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss  # Focal Loss formula\n",
    "        return torch.mean(F_loss)\n",
    "\n",
    "# Initialize the Focal Loss function with computed class weights\n",
    "loss_fct = FocalLoss(alpha=1, gamma=2, class_weights=class_weights)\n",
    "\n",
    "# Set up the optimizer with AdamW and a learning rate suitable for fine-tuning BERT\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "# Calculate the total number of training steps for the scheduler\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "\n",
    "# Reinitialize the optimizer with a learning rate suitable for fine-tuning BERT\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Set up a linear learning rate scheduler with warmup\n",
    "# Warmup steps are set to 10% of the total training steps to gradually increase the learning rate at the start\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(0.1 * num_training_steps), \n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94434b16b1274ab5bc25ab9a73ed57a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Loss: 0.2028, Accuracy: 0.6511, F1 Score: 0.6758\n",
      "Epoch 2: Validation Loss: 0.1899, Accuracy: 0.6181, F1 Score: 0.6460\n",
      "Epoch 3: Validation Loss: 0.1958, Accuracy: 0.5989, F1 Score: 0.6263\n",
      "Epoch 4: Validation Loss: 0.2453, Accuracy: 0.7115, F1 Score: 0.7254\n",
      "Epoch 5: Validation Loss: 0.2582, Accuracy: 0.7088, F1 Score: 0.7225\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    model.train()  # Set model to training mode\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}  # Move batch to the appropriate device\n",
    "        label_ids = batch['labels']\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "\n",
    "        # Forward pass to compute logits\n",
    "        logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Calculate loss using Focal Loss for improved handling of class imbalance\n",
    "        loss = loss_fct(logits, label_ids)\n",
    "\n",
    "        # Backward pass for gradient calculation and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        # Update model parameters and learning rate schedule\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Adjust learning rate\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        progress_bar.update(1)  # Update progress bar\n",
    "\n",
    "    # Validation Phase (run after each epoch to evaluate model performance)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_true_labels = []\n",
    "    all_predictions = []\n",
    "    total_val_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # No gradient calculation needed in evaluation\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            label_ids = batch['labels']\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "\n",
    "            # Forward pass to compute validation logits\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Calculate validation loss\n",
    "            loss = loss_fct(logits, label_ids)\n",
    "            total_val_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # Collect predictions and true labels for metrics calculation\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            all_true_labels.extend(label_ids.cpu().numpy())\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Calculate and display validation metrics for the epoch\n",
    "    avg_val_loss = total_val_loss / num_batches\n",
    "    accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "    f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "    print(f\"Epoch {epoch + 1}: Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Set Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Not Bot       0.85      0.70      0.77       278\n",
      "         Bot       0.38      0.60      0.47        87\n",
      "\n",
      "    accuracy                           0.67       365\n",
      "   macro avg       0.61      0.65      0.62       365\n",
      "weighted avg       0.74      0.67      0.69       365\n",
      "\n",
      "Test Set Accuracy: 0.6740\n",
      "Test Set Recall: 0.5977\n",
      "Test Set Precision: 0.3824\n",
      "Test Set F1-Score: 0.4664\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_true_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "# Run inference on the test set without computing gradients\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        logits = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        preds = torch.argmax(logits, dim=-1)  # Get predicted class\n",
    "\n",
    "        # Collect true labels and predictions for final metric calculations\n",
    "        all_true_labels.extend(batch['labels'].cpu().numpy())\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calculate and display evaluation metrics on the test set\n",
    "# Metrics include accuracy, recall, precision, F1-score, and a detailed classification report\n",
    "report = classification_report(all_true_labels, all_predictions, target_names=[\"Not Bot\", \"Bot\"])\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "recall = recall_score(all_true_labels, all_predictions)\n",
    "precision = precision_score(all_true_labels, all_predictions)\n",
    "f1 = f1_score(all_true_labels, all_predictions)\n",
    "\n",
    "print(\"Final Test Set Classification Report:\\n\", report)\n",
    "print(f\"Test Set Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Set Recall: {recall:.4f}\")\n",
    "print(f\"Test Set Precision: {precision:.4f}\")\n",
    "print(f\"Test Set F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the confusion matrix for the test set predictions\n",
    "cm = confusion_matrix(all_true_labels, all_predictions, labels=[0, 1])\n",
    "\n",
    "# Visualize the confusion matrix using a heatmap for clarity\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Not Bot\", \"Bot\"], yticklabels=[\"Not Bot\", \"Bot\"])\n",
    "\n",
    "# Label the axes and add a title for the plot\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix on Test Set')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
