{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"source data/twitter_human_bots_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# Check cuda status\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "                  \n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN in 'description': 7257\n",
      "Number of rows with NaN 'description' and account_type 'human': 2911\n",
      "Number of rows with NaN 'description' and account_type 'bot': 4346\n"
     ]
    }
   ],
   "source": [
    "missing_descriptions = df['description'].isna().sum()\n",
    "print(f\"Number of rows with NaN in 'description': {missing_descriptions}\")\n",
    "\n",
    "# Count rows where 'description' is NaN and 'label' is 'human'\n",
    "num_human = df[df['description'].isna() & (df['account_type'] == 'human')].shape[0]\n",
    "\n",
    "# Count rows where 'description' is NaN and 'label' is 'bot'\n",
    "num_bot = df[df['description'].isna() & (df['account_type'] == 'bot')].shape[0]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of rows with NaN 'description' and account_type 'human': {num_human}\")\n",
    "print(f\"Number of rows with NaN 'description' and account_type 'bot': {num_bot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Replace NaN values in 'description' with a placeholder string\n",
    "df['description'].fillna('unknown', inplace=True)\n",
    "print(df['description'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Convert account_type from categorical ('bot'/'human') to numerical labels (0/1)\n",
    "df['account_type'] = df['account_type'].map({'bot': 0, 'human': 1})\n",
    "\n",
    "# Convert pandas DataFrame to Hugging Face dataset format\n",
    "dataset = Dataset.from_pandas(df[['description', 'account_type']])\n",
    "\n",
    "# Perform an 80-20 train-test split with a fixed random seed\n",
    "train_test_split = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 29950/29950 [00:02<00:00, 10601.12 examples/s]\n",
      "Map: 100%|██████████| 7488/7488 [00:00<00:00, 14570.50 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 29950\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 7488\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"description\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "# Tokenize the training and testing datasets\n",
    "tokenized_train = train_test_split['train'].map(tokenize_function, batched=True)\n",
    "tokenized_test = train_test_split['test'].map(tokenize_function, batched=True)\n",
    "\n",
    "# Rename columns for consistency\n",
    "tokenized_train = tokenized_train.rename_column(\"account_type\", \"labels\")\n",
    "tokenized_test = tokenized_test.rename_column(\"account_type\", \"labels\")\n",
    "\n",
    "# Remove the 'description' column from the tokenized datasets\n",
    "tokenized_train = tokenized_train.remove_columns([\"description\"])\n",
    "tokenized_test = tokenized_test.remove_columns([\"description\"])\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train.set_format(\"torch\", columns=[\"labels\", \"input_ids\", \"token_type_ids\", \"attention_mask\"])\n",
    "tokenized_test.set_format(\"torch\", columns=[\"labels\", \"input_ids\", \"token_type_ids\", \"attention_mask\"])\n",
    "\n",
    "# Create a DatasetDict\n",
    "tokenized_datasets = DatasetDict({\n",
    "    \"train\": tokenized_train,\n",
    "    \"test\": tokenized_test\n",
    "})\n",
    "\n",
    "# Check the structure of the DatasetDict\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset with a fixed seed and select a range of examples\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000))\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batchsize=10\n",
    "# Create Dataloader\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batchsize)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "# Because we initialized BertForMaskedLM and concat is with our classifier instead of directly using BertForSequenceClassification\n",
    "# Some weights of the model checkpoint at bert-base-uncased were not used is within the expectation.\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,output_dim,dropout_rate):\n",
    "        super(Model,self).__init__()\n",
    "        self.encoder=AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\", output_hidden_states=True, return_dict=True)\n",
    "        self.dropout=nn.Dropout(dropout_rate)\n",
    "        # For the \"bert-base-uncased\" model, each hidden state has a dimension of 768.\n",
    "        # the value 3072=4*768 corresponds to the total dimension of the concatenated hidden states from the BERT model.\n",
    "        self.classifier=nn.Linear(3072,output_dim)\n",
    "\n",
    "\n",
    "    def forward(self,input_ids,token_type_ids,attention_mask):\n",
    "        outputs = self.encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # The BERT-base-uncased model has 12 hidden layers plus the initial embedding layer.\n",
    "        # This line concatenates the hidden states from the last four transformer layers from BERT along the last dimension.\n",
    "        # The shape of the resulting tensor is [batch_size, sequence_length, hidden_dim*4].\n",
    "        hidden_states = torch.cat(tuple([outputs.hidden_states[i] for i in [-1, -2, -3, -4]]), dim=-1)\n",
    "\n",
    "        # We are actually extracting the hidden state of the [CLS] token for each sequence in the batch.\n",
    "        # This [CLS] token's hidden state is typically used as a fixed-size representation of the entire sequence.\n",
    "        # This representation has been learned during BERT's pretraining to capture important information for various tasks.\n",
    "        # In the context of classification, you can think of the [CLS] token's hidden state as a summary of the sequence's content.\n",
    "        # Dropout is applied to this representation and is then fed into the linear classifier to make predictions for the task at hand.\n",
    "        x=self.dropout(hidden_states[:, 0, :])\n",
    "        x=self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "/Users/thiri/Downloads/bt4222/.venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import AdamW\n",
    "\n",
    "model = Model(output_dim=2, dropout_rate = 0.5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "loss_fct = CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "epochs = 5\n",
    "num_training_steps = epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "progress_bar = tqdm(range(num_training_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 200/1000 [06:02<28:33,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "val_loss : 0.6618937253952026\n",
      "val_accuracy: 72.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 400/1000 [14:39<33:04,  3.31s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "val_loss : 0.7385145425796509\n",
      "val_accuracy: 79.5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 600/1000 [22:00<13:31,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "val_loss : 0.7959121465682983\n",
      "val_accuracy: 76.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 800/1000 [28:53<07:29,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "val_loss : 1.0708229541778564\n",
      "val_accuracy: 79.0\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [35:54<00:00,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "val_loss : 1.0560492277145386\n",
      "val_accuracy: 79.5\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "        for batch in train_dataloader:\n",
    "            model.train()\n",
    "            # Loop through batches in the training data loader\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            label_ids = batch['labels'].long()\n",
    "            input_ids = batch['input_ids']\n",
    "            token_type_ids = None\n",
    "            # When using BERT for tasks like single-text classification or sequence labeling, the token_type_ids is an optional parameter, commonly set to None.\n",
    "            attention_mask = batch['attention_mask']\n",
    "            # Perform a forward pass through the model to get logits\n",
    "            logits = model(input_ids, token_type_ids, attention_mask)\n",
    "\n",
    "            # Calculate the loss using the provided loss function\n",
    "            loss = loss_fct(logits, label_ids.view(-1))\n",
    "            # Perform backward pass and update model parameters\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad() # Clear accumulated gradients\n",
    "            progress_bar.update(1) # Update progress bar\n",
    "\n",
    "        # Set the model to evaluation mode for validation\n",
    "        model.eval()\n",
    "        for batch in eval_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad(): # disable gradient computation\n",
    "                label_ids = batch['labels']\n",
    "                input_ids = batch['input_ids']\n",
    "                token_type_ids = None\n",
    "                attention_mask = batch['attention_mask']\n",
    "                logits = model(input_ids, token_type_ids, attention_mask)\n",
    "                loss = loss_fct(logits, label_ids.view(-1))\n",
    "\n",
    "            # Get predicted labels by selecting the class with the highest probability\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "        acc = metric.compute()\n",
    "        print(f'Epoch {epoch+1}')\n",
    "        print(f'val_loss : {loss}')\n",
    "        print(f\"val_accuracy: {acc['accuracy'] * 100}\")\n",
    "        print(25*'==')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
